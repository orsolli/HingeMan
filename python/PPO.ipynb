{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 1e7 # Set maximum number of steps to run environment.\n",
    "run_path = \"deepHinge\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 30000 # Frequency at which to save training statistics.\n",
    "save_freq = 120000 # Frequency at which to save model.\n",
    "env_name = \"build/human\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.98 # Reward discount rate.\n",
    "lambd = 0.935 # Lambda parameter for GAE.\n",
    "time_horizon = 256 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-2 # Strength of entropy regularization\n",
    "num_epoch = 6 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 6 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.125 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 65536 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 15e-4 # Model learning rate.\n",
    "hidden_units = 12 # Number of units in hidden layer.\n",
    "batch_size = 1024 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 88\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 21\n",
      "        Memory space size (per agent): 128\n",
      "        Action descriptions: hode side, hode rist, hode nikk, rygg bukk, rygg len, l?r v l?p, l?r v strafe, legg v, fot v, l?r h l?p, l?r h strafe, legg h, fot h, skulder v klem, skulder v l?ft, arm v l?ft, arm v vri, skulder h klem, skulder h l?ft, arm h l?ft, arm h vri\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 30000. Mean Reward: -14.775335307301694. Std of Reward: 26.67167739261543.\n",
      "Step: 60000. Mean Reward: -19.607164205464983. Std of Reward: 27.47984548726326.\n",
      "Step: 90000. Mean Reward: 9.408647110759562. Std of Reward: 14.558772796438758.\n",
      "Step: 120000. Mean Reward: 13.615011747795219. Std of Reward: 6.096107427388137.\n",
      "Saved Model\n",
      "Step: 150000. Mean Reward: 24.49855257717239. Std of Reward: 11.32776470729965.\n",
      "Step: 180000. Mean Reward: 32.88825581378073. Std of Reward: 5.62587707014296.\n",
      "Step: 210000. Mean Reward: 35.876598517080886. Std of Reward: 9.183298504299858.\n",
      "Step: 240000. Mean Reward: 42.71420879253553. Std of Reward: 10.538794222478243.\n",
      "Saved Model\n",
      "Step: 270000. Mean Reward: 37.677737587439424. Std of Reward: 13.425552644029967.\n",
      "Step: 300000. Mean Reward: 20.091243063125173. Std of Reward: 7.419099754198937.\n",
      "Step: 330000. Mean Reward: 19.85184823773228. Std of Reward: 7.408426685743528.\n",
      "Step: 360000. Mean Reward: 48.27493939332521. Std of Reward: 18.022777908842027.\n",
      "Saved Model\n",
      "Step: 390000. Mean Reward: 47.64240779694382. Std of Reward: 17.220396061801576.\n",
      "Step: 420000. Mean Reward: 87.46421693677537. Std of Reward: 29.45046635141129.\n",
      "Step: 450000. Mean Reward: 104.23734329253773. Std of Reward: 15.38982574221339.\n",
      "Step: 480000. Mean Reward: 91.95827756721746. Std of Reward: 16.403835081292723.\n",
      "Saved Model\n",
      "Step: 510000. Mean Reward: 84.83025716489445. Std of Reward: 16.978892227067504.\n",
      "Step: 540000. Mean Reward: 60.51656892356931. Std of Reward: 31.249421695354872.\n",
      "Step: 570000. Mean Reward: 27.661890276197855. Std of Reward: 6.970505783496584.\n",
      "Step: 600000. Mean Reward: 23.391978065170726. Std of Reward: 16.665580605850625.\n",
      "Saved Model\n",
      "Step: 630000. Mean Reward: -15.826324760901636. Std of Reward: 30.639441263018753.\n",
      "Step: 660000. Mean Reward: -16.213378715725764. Std of Reward: 29.80228257361162.\n",
      "Step: 690000. Mean Reward: -11.198439534564818. Std of Reward: 29.61122799586812.\n",
      "Step: 720000. Mean Reward: -11.412138578834073. Std of Reward: 25.515955077023808.\n",
      "Saved Model\n",
      "Step: 750000. Mean Reward: -8.927468636089147. Std of Reward: 30.671627318117856.\n",
      "Step: 780000. Mean Reward: -8.246369162288124. Std of Reward: 31.60615572408306.\n",
      "Step: 810000. Mean Reward: -9.269794044624465. Std of Reward: 29.720103483980683.\n",
      "Step: 840000. Mean Reward: -3.734981203117968. Std of Reward: 33.124733994796.\n",
      "Saved Model\n",
      "Step: 870000. Mean Reward: -9.901485766673176. Std of Reward: 23.994476138772395.\n",
      "Step: 900000. Mean Reward: -10.321432967267617. Std of Reward: 1.5845415587512415.\n",
      "Step: 930000. Mean Reward: -7.364692085371639. Std of Reward: 14.913302929387486.\n",
      "Step: 960000. Mean Reward: 60.84129943083252. Std of Reward: 28.450052238152143.\n",
      "Saved Model\n",
      "Step: 990000. Mean Reward: 60.59346201022389. Std of Reward: 23.53530413806357.\n",
      "Step: 1020000. Mean Reward: 10.227399184088249. Std of Reward: 28.513014527589057.\n",
      "Step: 1050000. Mean Reward: 5.592239477163864. Std of Reward: 25.951013155331886.\n",
      "Step: 1080000. Mean Reward: 49.49945423954793. Std of Reward: 32.61494193855711.\n",
      "Saved Model\n",
      "Step: 1110000. Mean Reward: 66.64093968061671. Std of Reward: 8.501539260329963.\n",
      "Step: 1140000. Mean Reward: 91.5784921137469. Std of Reward: 36.023387032113995.\n",
      "Step: 1170000. Mean Reward: 119.85324059956278. Std of Reward: 22.839237613861602.\n",
      "Step: 1200000. Mean Reward: 109.20440140196644. Std of Reward: 31.566964567183057.\n",
      "Saved Model\n",
      "Step: 1230000. Mean Reward: 80.50894163706322. Std of Reward: 17.79403942595767.\n",
      "Step: 1260000. Mean Reward: 75.39152881648539. Std of Reward: 19.74311757501324.\n",
      "Step: 1290000. Mean Reward: 45.662091178263225. Std of Reward: 22.06129388843539.\n",
      "Step: 1320000. Mean Reward: 42.54922075889347. Std of Reward: 22.518268309929823.\n",
      "Saved Model\n",
      "Step: 1350000. Mean Reward: 95.14182470109948. Std of Reward: 30.203581748672256.\n",
      "Step: 1380000. Mean Reward: 104.40415964076928. Std of Reward: 23.09155675309574.\n",
      "Step: 1410000. Mean Reward: 84.30025515347658. Std of Reward: 23.493720227334556.\n",
      "Step: 1440000. Mean Reward: 79.09392971802215. Std of Reward: 16.37428291934055.\n",
      "Saved Model\n",
      "Step: 1470000. Mean Reward: 103.27142072849377. Std of Reward: 36.76612458909155.\n",
      "Step: 1500000. Mean Reward: 126.83552603463852. Std of Reward: 29.015002875138386.\n",
      "Step: 1530000. Mean Reward: 104.46681839802092. Std of Reward: 49.58408975642446.\n",
      "Step: 1560000. Mean Reward: 50.88667673560816. Std of Reward: 38.73116284794614.\n",
      "Saved Model\n",
      "Step: 1590000. Mean Reward: 53.569116290665185. Std of Reward: 35.62614667927924.\n",
      "Step: 1620000. Mean Reward: 88.37064761728718. Std of Reward: 17.088222740915246.\n",
      "Step: 1650000. Mean Reward: 87.60252535685157. Std of Reward: 17.808413397520273.\n",
      "Step: 1680000. Mean Reward: 128.86118772512066. Std of Reward: 30.49015061575678.\n",
      "Saved Model\n",
      "Step: 1710000. Mean Reward: 132.97476354530752. Std of Reward: 27.616292469964268.\n",
      "Step: 1740000. Mean Reward: 138.94488091567897. Std of Reward: 25.4337906016647.\n",
      "Step: 1770000. Mean Reward: 141.52630562917156. Std of Reward: 24.62371087954298.\n",
      "Step: 1800000. Mean Reward: 116.84001417552192. Std of Reward: 31.314238081260072.\n",
      "Saved Model\n",
      "Step: 1830000. Mean Reward: 96.33588532607583. Std of Reward: 18.780079673124092.\n",
      "Step: 1860000. Mean Reward: 103.05025702504408. Std of Reward: 23.08086549326049.\n",
      "Step: 1890000. Mean Reward: 121.3864297725771. Std of Reward: 31.436233701046806.\n",
      "Step: 1920000. Mean Reward: 123.18836129819017. Std of Reward: 30.30866708104397.\n",
      "Saved Model\n",
      "Step: 1950000. Mean Reward: 129.13187925263384. Std of Reward: 27.338772060399048.\n",
      "Step: 1980000. Mean Reward: 129.72501675831907. Std of Reward: 32.215158981791106.\n",
      "Step: 2010000. Mean Reward: 120.92597534386955. Std of Reward: 29.773366832463537.\n",
      "Step: 2040000. Mean Reward: 120.29901868743507. Std of Reward: 30.316163973916296.\n",
      "Saved Model\n",
      "Step: 2070000. Mean Reward: 92.6081182869035. Std of Reward: 34.66214790823822.\n",
      "Step: 2100000. Mean Reward: 83.08318584315627. Std of Reward: 30.665169610690477.\n",
      "Step: 2130000. Mean Reward: 61.25653984819422. Std of Reward: 36.44181461711184.\n",
      "Step: 2160000. Mean Reward: 41.62159396360593. Std of Reward: 30.29982954101066.\n",
      "Saved Model\n",
      "Step: 2190000. Mean Reward: 33.52094779737411. Std of Reward: 31.919452058561266.\n",
      "Step: 2220000. Mean Reward: 21.518462838925153. Std of Reward: 26.735169718664505.\n",
      "Step: 2250000. Mean Reward: 27.383886633053528. Std of Reward: 38.499838747988235.\n",
      "Step: 2280000. Mean Reward: 108.14215129190876. Std of Reward: 42.82922172502365.\n",
      "Saved Model\n",
      "Step: 2310000. Mean Reward: 108.51364995986997. Std of Reward: 40.553570952842854.\n",
      "Step: 2340000. Mean Reward: 84.13495572873656. Std of Reward: 29.77209341565076.\n",
      "Step: 2370000. Mean Reward: 85.24719916131484. Std of Reward: 28.105885399512257.\n",
      "Step: 2400000. Mean Reward: 49.4159412375587. Std of Reward: 23.858677316438285.\n",
      "Saved Model\n",
      "Step: 2430000. Mean Reward: 38.777551299931616. Std of Reward: 3.7515906031767003.\n",
      "Step: 2460000. Mean Reward: 38.68240461759481. Std of Reward: 3.94913661186467.\n",
      "Step: 2490000. Mean Reward: 38.61247497163305. Std of Reward: 3.6873666626263417.\n",
      "Step: 2520000. Mean Reward: 38.563955362226615. Std of Reward: 3.41215904841977.\n",
      "Saved Model\n",
      "Step: 2550000. Mean Reward: 37.765209032478744. Std of Reward: 3.789300848749621.\n",
      "Step: 2580000. Mean Reward: 38.616340132873354. Std of Reward: 4.718992680782864.\n",
      "Step: 2610000. Mean Reward: 48.69104470496487. Std of Reward: 5.992700720996351.\n",
      "Step: 2640000. Mean Reward: 48.1163386473445. Std of Reward: 6.285822080074336.\n",
      "Saved Model\n",
      "Step: 2670000. Mean Reward: 111.78336791797071. Std of Reward: 36.47287532089306.\n",
      "Step: 2700000. Mean Reward: 127.10910102196752. Std of Reward: 19.243138760473744.\n",
      "Step: 2730000. Mean Reward: 130.87143677106764. Std of Reward: 19.40051442904539.\n",
      "Step: 2760000. Mean Reward: 129.93148743978406. Std of Reward: 17.53682128314199.\n",
      "Saved Model\n",
      "Step: 2790000. Mean Reward: 122.19610226162966. Std of Reward: 23.09403422884715.\n",
      "Step: 2820000. Mean Reward: 113.60869053194132. Std of Reward: 23.35036518687984.\n",
      "Step: 2850000. Mean Reward: 117.23957371719945. Std of Reward: 22.415742201335707.\n",
      "Step: 2880000. Mean Reward: 132.12510716606164. Std of Reward: 21.633746258902015.\n",
      "Saved Model\n",
      "Step: 2910000. Mean Reward: 123.98370966138891. Std of Reward: 30.781392636551963.\n",
      "Step: 2940000. Mean Reward: 56.046079728733204. Std of Reward: 33.65277556849616.\n",
      "Step: 2970000. Mean Reward: 53.76162315506205. Std of Reward: 31.567384330632237.\n",
      "Step: 3000000. Mean Reward: 58.24735917253199. Std of Reward: 33.102925027283106.\n",
      "Saved Model\n",
      "Step: 3030000. Mean Reward: 55.41408550272959. Std of Reward: 31.39321912900193.\n",
      "Step: 3060000. Mean Reward: 29.148783582516053. Std of Reward: 35.25169246839047.\n",
      "Step: 3090000. Mean Reward: 14.47115194629006. Std of Reward: 29.29536225330162.\n",
      "Step: 3120000. Mean Reward: 22.40280710342514. Std of Reward: 30.65141089310514.\n",
      "Saved Model\n",
      "Step: 3150000. Mean Reward: 30.257514956643742. Std of Reward: 23.567232347477866.\n",
      "Step: 3180000. Mean Reward: 28.890073841251304. Std of Reward: 31.250754565394463.\n",
      "Step: 3210000. Mean Reward: 26.664075872271837. Std of Reward: 41.45838410759445.\n",
      "Step: 3240000. Mean Reward: 27.017753549632165. Std of Reward: 38.68541956200867.\n",
      "Saved Model\n",
      "Step: 3270000. Mean Reward: 34.139333867631336. Std of Reward: 29.15207282404913.\n",
      "Step: 3300000. Mean Reward: 30.412722669008854. Std of Reward: 26.728438235022573.\n",
      "Step: 3330000. Mean Reward: 26.429709591309237. Std of Reward: 38.331375941562584.\n",
      "Step: 3360000. Mean Reward: 27.787818737603622. Std of Reward: 43.765326777919896.\n",
      "Saved Model\n",
      "Step: 3390000. Mean Reward: 45.891712731204365. Std of Reward: 42.17999971591939.\n",
      "Step: 3420000. Mean Reward: 42.66362153272944. Std of Reward: 40.18506479612936.\n",
      "Step: 3450000. Mean Reward: 44.48692251711219. Std of Reward: 42.087303328397624.\n",
      "Step: 3480000. Mean Reward: 35.3445551734655. Std of Reward: 38.45182315534694.\n",
      "Saved Model\n",
      "Step: 3510000. Mean Reward: 44.592593080880306. Std of Reward: 32.53452074575156.\n",
      "Step: 3540000. Mean Reward: 62.78880458010398. Std of Reward: 18.38947525181465.\n",
      "Step: 3570000. Mean Reward: 61.15368477039935. Std of Reward: 23.893187769967206.\n",
      "Step: 3600000. Mean Reward: 75.7651171660439. Std of Reward: 41.602179983284245.\n",
      "Saved Model\n",
      "Step: 3630000. Mean Reward: 73.27018643074746. Std of Reward: 46.05508024919161.\n",
      "Step: 3660000. Mean Reward: 72.73910597970828. Std of Reward: 16.19344560807545.\n",
      "Step: 3690000. Mean Reward: 73.21774279144293. Std of Reward: 10.38178925792924.\n",
      "Step: 3720000. Mean Reward: 66.68345911792821. Std of Reward: 14.772868955325942.\n",
      "Saved Model\n",
      "Step: 3750000. Mean Reward: 67.18188596507756. Std of Reward: 14.698795921894748.\n",
      "Step: 3780000. Mean Reward: 56.6885135344628. Std of Reward: 14.281612921464212.\n",
      "Step: 3810000. Mean Reward: 50.17978527890442. Std of Reward: 3.9305772258763976.\n",
      "Step: 3840000. Mean Reward: 48.463675514150445. Std of Reward: 4.644655014938621.\n",
      "Saved Model\n",
      "Step: 3870000. Mean Reward: 44.14951481117669. Std of Reward: 4.3448611604434575.\n",
      "Step: 3900000. Mean Reward: 44.80770762711283. Std of Reward: 4.906257212380341.\n",
      "Step: 3930000. Mean Reward: 54.41408655771676. Std of Reward: 2.9822860098847235.\n",
      "Step: 3960000. Mean Reward: 54.465833731951605. Std of Reward: 2.964715966381309.\n",
      "Saved Model\n",
      "Step: 3990000. Mean Reward: 57.81055975984529. Std of Reward: 5.6096893117774815.\n",
      "Step: 4020000. Mean Reward: 59.16931069152644. Std of Reward: 6.235515306707767.\n",
      "Step: 4050000. Mean Reward: 53.395508645915335. Std of Reward: 11.54387249120899.\n",
      "Step: 4080000. Mean Reward: 47.300999819821996. Std of Reward: 13.438701933720635.\n",
      "Saved Model\n",
      "Step: 4110000. Mean Reward: 48.16145707667919. Std of Reward: 11.742038657979625.\n",
      "Step: 4140000. Mean Reward: 49.26813704055885. Std of Reward: 12.075604883340324.\n",
      "Step: 4170000. Mean Reward: 47.3436702840621. Std of Reward: 11.80426324851337.\n",
      "Step: 4200000. Mean Reward: 44.51727699820916. Std of Reward: 14.24590253328459.\n",
      "Saved Model\n",
      "Step: 4230000. Mean Reward: 44.50226480324406. Std of Reward: 12.867529321200237.\n",
      "Step: 4260000. Mean Reward: 41.50275464253779. Std of Reward: 12.242474780630216.\n",
      "Step: 4290000. Mean Reward: 46.342162560139165. Std of Reward: 68.66460568427.\n",
      "Step: 4320000. Mean Reward: 38.18247926831633. Std of Reward: 13.512073161635978.\n",
      "Saved Model\n",
      "Step: 4350000. Mean Reward: 38.91689898858246. Std of Reward: 15.118895668016567.\n",
      "Step: 4380000. Mean Reward: 40.18030334729139. Std of Reward: 12.780266120017943.\n",
      "Step: 4410000. Mean Reward: 42.61967957514498. Std of Reward: 12.137931918044764.\n",
      "Step: 4440000. Mean Reward: 45.571764266848696. Std of Reward: 12.127272361695896.\n",
      "Saved Model\n",
      "Step: 4470000. Mean Reward: 48.957549161384584. Std of Reward: 13.088607353854202.\n",
      "Step: 4500000. Mean Reward: 52.44998187519154. Std of Reward: 16.771411419181426.\n",
      "Step: 4530000. Mean Reward: 61.803861173232605. Std of Reward: 14.755628531864827.\n",
      "Step: 4560000. Mean Reward: 60.05560187521966. Std of Reward: 11.138070738554525.\n",
      "Saved Model\n",
      "Step: 4590000. Mean Reward: 64.15824079292287. Std of Reward: 12.476928324614429.\n",
      "Step: 4620000. Mean Reward: 64.41474270064626. Std of Reward: 14.58788245501358.\n",
      "Step: 4650000. Mean Reward: 67.62880102187006. Std of Reward: 15.99843769451541.\n",
      "Step: 4680000. Mean Reward: 66.89367001694244. Std of Reward: 12.031908866168354.\n",
      "Saved Model\n",
      "Step: 4710000. Mean Reward: 57.59298299568379. Std of Reward: 17.731753983580955.\n",
      "Step: 4740000. Mean Reward: 49.78552702706654. Std of Reward: 14.450217692845987.\n",
      "Step: 4770000. Mean Reward: 52.46960684133114. Std of Reward: 15.546558469678853.\n",
      "Step: 4800000. Mean Reward: 60.21992157624005. Std of Reward: 11.104008332705462.\n",
      "Saved Model\n",
      "Step: 4830000. Mean Reward: 55.876196531036705. Std of Reward: 14.178379086139364.\n",
      "Step: 4860000. Mean Reward: 45.89170421907655. Std of Reward: 13.322210916612635.\n",
      "Step: 4890000. Mean Reward: 45.45594375778358. Std of Reward: 15.07077755002196.\n",
      "Step: 4920000. Mean Reward: 47.97598037080471. Std of Reward: 14.558033384400703.\n",
      "Saved Model\n",
      "Step: 4950000. Mean Reward: 47.02527612074677. Std of Reward: 13.271110785805554.\n",
      "Step: 4980000. Mean Reward: 56.8034631498687. Std of Reward: 11.299692849705284.\n",
      "Step: 5010000. Mean Reward: 59.79261408728855. Std of Reward: 6.299042665838372.\n",
      "Step: 5040000. Mean Reward: 52.50512971188529. Std of Reward: 13.742572030938094.\n",
      "Saved Model\n",
      "Step: 5070000. Mean Reward: 46.62852871909193. Std of Reward: 13.108688917068182.\n",
      "Step: 5100000. Mean Reward: 47.23882438724485. Std of Reward: 13.090734010231253.\n",
      "Step: 5130000. Mean Reward: 50.359916748833626. Std of Reward: 12.620511845353132.\n",
      "Step: 5160000. Mean Reward: 50.068193467315695. Std of Reward: 11.6406183146055.\n",
      "Saved Model\n",
      "Step: 5190000. Mean Reward: 52.51969501512443. Std of Reward: 9.422445861264848.\n",
      "Step: 5220000. Mean Reward: 52.51355697756961. Std of Reward: 8.743264313107026.\n",
      "Step: 5250000. Mean Reward: 53.31000372390655. Std of Reward: 8.078973208224426.\n",
      "Step: 5280000. Mean Reward: 52.968839640667824. Std of Reward: 7.981048501227367.\n",
      "Saved Model\n",
      "Step: 5310000. Mean Reward: 56.47320145231502. Std of Reward: 6.467038265575082.\n",
      "Step: 5340000. Mean Reward: 56.93011470270215. Std of Reward: 6.0812868490318515.\n",
      "Step: 5370000. Mean Reward: 49.7262522055382. Std of Reward: 12.522830342477915.\n",
      "Step: 5400000. Mean Reward: 43.488658209161514. Std of Reward: 13.745219484497099.\n",
      "Saved Model\n",
      "Step: 5430000. Mean Reward: 43.15754607647336. Std of Reward: 16.014787127520318.\n",
      "Step: 5460000. Mean Reward: 46.103176559065254. Std of Reward: 15.663185033736136.\n",
      "Step: 5490000. Mean Reward: 47.85676826885545. Std of Reward: 16.092194741102958.\n",
      "Step: 5520000. Mean Reward: 56.65191087714489. Std of Reward: 16.38274687893759.\n",
      "Saved Model\n",
      "Step: 5550000. Mean Reward: 55.19664592351066. Std of Reward: 13.223263589489473.\n",
      "Step: 5580000. Mean Reward: 68.96613559742177. Std of Reward: 13.607163342630017.\n",
      "Step: 5610000. Mean Reward: 70.21589366785062. Std of Reward: 18.86957565034296.\n",
      "Step: 5640000. Mean Reward: 61.788329881998536. Std of Reward: 15.357509978637946.\n",
      "Saved Model\n",
      "Step: 5670000. Mean Reward: 59.62133204374589. Std of Reward: 15.634529597061261.\n",
      "Step: 5700000. Mean Reward: 56.268222322400284. Std of Reward: 17.662620062307447.\n",
      "Step: 5730000. Mean Reward: 55.24470923995407. Std of Reward: 15.221786900150239.\n",
      "Step: 5760000. Mean Reward: 56.93009141811379. Std of Reward: 13.12541503169969.\n",
      "Saved Model\n",
      "Step: 5790000. Mean Reward: 65.1627420385065. Std of Reward: 13.451743123400176.\n",
      "Step: 5820000. Mean Reward: 63.96995151934918. Std of Reward: 13.211414964139584.\n",
      "Step: 5850000. Mean Reward: 58.24165561726871. Std of Reward: 14.146477583148565.\n",
      "Step: 5880000. Mean Reward: 58.97284991189745. Std of Reward: 16.624176868359033.\n",
      "Saved Model\n",
      "Step: 5910000. Mean Reward: 56.1845341490133. Std of Reward: 17.2040564301905.\n",
      "Step: 5940000. Mean Reward: 55.1347447171179. Std of Reward: 14.258779963876444.\n",
      "Step: 5970000. Mean Reward: 61.97285685985666. Std of Reward: 15.405738838694319.\n",
      "Step: 6000000. Mean Reward: 63.999268950183605. Std of Reward: 13.923329628794427.\n",
      "Saved Model\n",
      "Step: 6030000. Mean Reward: 64.20900744476182. Std of Reward: 13.841045466673817.\n",
      "Step: 6060000. Mean Reward: 63.52095086263224. Std of Reward: 13.362994440354646.\n",
      "Step: 6090000. Mean Reward: 56.95447163040235. Std of Reward: 17.70792041835561.\n",
      "Step: 6120000. Mean Reward: 44.02238809692788. Std of Reward: 14.198753553061245.\n",
      "Saved Model\n",
      "Step: 6150000. Mean Reward: 45.09478722547443. Std of Reward: 12.349279666209778.\n",
      "Step: 6180000. Mean Reward: 54.1697529002418. Std of Reward: 11.062368379048198.\n",
      "Step: 6210000. Mean Reward: 53.791378618106734. Std of Reward: 10.483137457236698.\n",
      "Step: 6240000. Mean Reward: 50.13096645479124. Std of Reward: 11.66962793109889.\n",
      "Saved Model\n",
      "Step: 6270000. Mean Reward: 48.652523003588534. Std of Reward: 12.969409682395815.\n",
      "Step: 6300000. Mean Reward: 55.07297135323707. Std of Reward: 10.995944694599403.\n",
      "Step: 6330000. Mean Reward: 56.55852767326187. Std of Reward: 8.357334246452917.\n",
      "Step: 6360000. Mean Reward: 56.191133130504284. Std of Reward: 8.706141134995137.\n",
      "Saved Model\n",
      "Step: 6390000. Mean Reward: 55.41466931034961. Std of Reward: 7.416397215357871.\n",
      "Step: 6420000. Mean Reward: 57.25696988389769. Std of Reward: 7.302472141283283.\n",
      "Step: 6450000. Mean Reward: 58.55363548715113. Std of Reward: 5.585968681889103.\n",
      "Step: 6480000. Mean Reward: 58.989227673152584. Std of Reward: 6.651935047892404.\n",
      "Saved Model\n",
      "Step: 6510000. Mean Reward: 60.64449534629434. Std of Reward: 6.368628909533208.\n",
      "Step: 6540000. Mean Reward: 60.21538285545359. Std of Reward: 5.8490458922431365.\n",
      "Step: 6570000. Mean Reward: 63.338356607617726. Std of Reward: 5.718895883190394.\n",
      "Step: 6600000. Mean Reward: 64.69549526838088. Std of Reward: 7.217646519210311.\n",
      "Saved Model\n",
      "Step: 6630000. Mean Reward: 63.87662470620095. Std of Reward: 6.290211767751223.\n",
      "Step: 6660000. Mean Reward: 63.34514806165957. Std of Reward: 5.192970925974323.\n",
      "Step: 6690000. Mean Reward: 58.73239646728085. Std of Reward: 7.783709382918118.\n",
      "Step: 6720000. Mean Reward: 55.62411665527394. Std of Reward: 6.416159173739145.\n",
      "Saved Model\n",
      "Step: 6750000. Mean Reward: 55.61481719897765. Std of Reward: 7.109260846592915.\n",
      "Step: 6780000. Mean Reward: 57.00506430737401. Std of Reward: 6.893878457460776.\n",
      "Step: 6810000. Mean Reward: 56.1647709271739. Std of Reward: 7.060905838222659.\n",
      "Step: 6840000. Mean Reward: 49.9807570709352. Std of Reward: 7.337558157462675.\n",
      "Saved Model\n",
      "Step: 6870000. Mean Reward: 50.09264435354727. Std of Reward: 7.142211715208722.\n",
      "Step: 6900000. Mean Reward: 55.85735473431536. Std of Reward: 8.90105085319627.\n",
      "Step: 6930000. Mean Reward: 56.609239850383034. Std of Reward: 7.8616737265913805.\n",
      "Step: 6960000. Mean Reward: 57.82461194801193. Std of Reward: 7.857142204911633.\n",
      "Saved Model\n",
      "Step: 6990000. Mean Reward: 56.10308254216032. Std of Reward: 8.711589636088512.\n",
      "Step: 7020000. Mean Reward: 56.47714991431503. Std of Reward: 8.738028939116859.\n",
      "Step: 7050000. Mean Reward: 54.389538896894315. Std of Reward: 9.412700735555067.\n",
      "Step: 7080000. Mean Reward: 54.86372378248309. Std of Reward: 8.786656859423715.\n",
      "Saved Model\n",
      "Step: 7110000. Mean Reward: 53.88809975190074. Std of Reward: 9.432213791206877.\n",
      "Step: 7140000. Mean Reward: 54.457570697566155. Std of Reward: 9.733180259878381.\n",
      "Step: 7170000. Mean Reward: 65.4386235338067. Std of Reward: 11.466226755109423.\n",
      "Step: 7200000. Mean Reward: 65.16120188124376. Std of Reward: 9.400979663736862.\n",
      "Saved Model\n",
      "Step: 7230000. Mean Reward: 62.50468946161038. Std of Reward: 12.245943313158051.\n",
      "Step: 7260000. Mean Reward: 61.23089285718625. Std of Reward: 8.657013561052809.\n",
      "Step: 7290000. Mean Reward: 61.02088695574799. Std of Reward: 10.635542984490876.\n",
      "Step: 7320000. Mean Reward: 57.949151721272536. Std of Reward: 11.195332991209208.\n",
      "Saved Model\n",
      "Step: 7350000. Mean Reward: 57.83229143095684. Std of Reward: 10.976367463545504.\n",
      "Step: 7380000. Mean Reward: 57.17561724153312. Std of Reward: 11.595736758807169.\n",
      "Step: 7410000. Mean Reward: 58.60227025980587. Std of Reward: 13.407189058404548.\n",
      "Step: 7440000. Mean Reward: 59.47692553779235. Std of Reward: 9.850479319713225.\n",
      "Saved Model\n",
      "Step: 7470000. Mean Reward: 57.911360571838934. Std of Reward: 10.22985165286541.\n",
      "Step: 7500000. Mean Reward: 54.22642115103324. Std of Reward: 8.698069785680675.\n",
      "Step: 7530000. Mean Reward: 54.53826941930039. Std of Reward: 8.374634588315226.\n",
      "Step: 7560000. Mean Reward: 55.9864599666917. Std of Reward: 7.1735809042720895.\n",
      "Saved Model\n",
      "Step: 7590000. Mean Reward: 56.47015429972424. Std of Reward: 7.46685366355533.\n",
      "Step: 7620000. Mean Reward: 58.07224645310811. Std of Reward: 6.397136256722797.\n",
      "Step: 7650000. Mean Reward: 58.193074510385934. Std of Reward: 6.354135378421179.\n",
      "Step: 7680000. Mean Reward: 61.069438360866414. Std of Reward: 5.727356822552237.\n",
      "Saved Model\n",
      "Step: 7710000. Mean Reward: 59.86901806152656. Std of Reward: 5.868940511142861.\n",
      "Step: 7740000. Mean Reward: 60.12897204094852. Std of Reward: 5.202627609417167.\n",
      "Step: 7770000. Mean Reward: 60.12725880781724. Std of Reward: 6.9545389265696835.\n",
      "Step: 7800000. Mean Reward: 59.73688055913738. Std of Reward: 5.883145338585226.\n",
      "Saved Model\n",
      "Step: 7830000. Mean Reward: 58.5238128988387. Std of Reward: 6.5232237751424735.\n",
      "Step: 7860000. Mean Reward: 59.052261223524845. Std of Reward: 6.141011840371684.\n",
      "Step: 7890000. Mean Reward: 58.115633792601294. Std of Reward: 6.613988230900481.\n",
      "Step: 7920000. Mean Reward: 57.809590945409816. Std of Reward: 6.450000433101738.\n",
      "Saved Model\n",
      "Step: 7950000. Mean Reward: 55.35000462931789. Std of Reward: 7.834115775534202.\n",
      "Step: 7980000. Mean Reward: 54.22477714731491. Std of Reward: 8.037635607635751.\n",
      "Step: 8010000. Mean Reward: 54.977290733117336. Std of Reward: 8.836677044466606.\n",
      "Step: 8040000. Mean Reward: 55.57423490726887. Std of Reward: 7.60572555106724.\n",
      "Saved Model\n",
      "Step: 8070000. Mean Reward: 55.98950116966319. Std of Reward: 7.5645148594418625.\n",
      "Step: 8100000. Mean Reward: 55.197761258343405. Std of Reward: 9.21361696328117.\n",
      "Step: 8130000. Mean Reward: 55.96309624616338. Std of Reward: 8.529617680811244.\n",
      "Step: 8160000. Mean Reward: 54.997230431170514. Std of Reward: 8.592539722742343.\n",
      "Saved Model\n",
      "Step: 8190000. Mean Reward: 54.59740152490913. Std of Reward: 9.000576172145932.\n",
      "Step: 8220000. Mean Reward: 57.31172901442335. Std of Reward: 8.99588149326264.\n",
      "Step: 8250000. Mean Reward: 56.18288116599471. Std of Reward: 8.966051269937568.\n",
      "Step: 8280000. Mean Reward: 56.52956402626273. Std of Reward: 9.274041560786545.\n",
      "Saved Model\n",
      "Step: 8310000. Mean Reward: 58.653485052957414. Std of Reward: 9.803414910019324.\n",
      "Step: 8340000. Mean Reward: 57.728257988638845. Std of Reward: 8.537538715760919.\n",
      "Step: 8370000. Mean Reward: 58.02904245057394. Std of Reward: 7.69726373652882.\n",
      "Step: 8400000. Mean Reward: 56.456652657114404. Std of Reward: 8.077277044867289.\n",
      "Saved Model\n",
      "Step: 8430000. Mean Reward: 55.94804610526873. Std of Reward: 7.944995674309073.\n",
      "Step: 8460000. Mean Reward: 55.82183344905091. Std of Reward: 7.645874399873303.\n",
      "Step: 8490000. Mean Reward: 55.46890790131112. Std of Reward: 8.407622110284791.\n",
      "Step: 8520000. Mean Reward: 55.63319770372777. Std of Reward: 8.49568717814869.\n",
      "Saved Model\n",
      "Step: 8550000. Mean Reward: 55.9876177062725. Std of Reward: 8.517923878591303.\n",
      "Step: 8580000. Mean Reward: 57.0783143353689. Std of Reward: 8.354863372868122.\n",
      "Step: 8610000. Mean Reward: 58.01787480822412. Std of Reward: 7.558154385764171.\n",
      "Step: 8640000. Mean Reward: 57.06905268093733. Std of Reward: 8.200029550997424.\n",
      "Saved Model\n",
      "Step: 8670000. Mean Reward: 56.73068681912304. Std of Reward: 8.159794478379304.\n",
      "Step: 8700000. Mean Reward: 57.48176990422244. Std of Reward: 8.549774104423314.\n",
      "Step: 8730000. Mean Reward: 55.47795896074566. Std of Reward: 9.038061933932477.\n",
      "Step: 8760000. Mean Reward: 57.78469738069244. Std of Reward: 8.973119192843633.\n",
      "Saved Model\n",
      "Step: 8790000. Mean Reward: 58.846551946135946. Std of Reward: 8.737230200102864.\n",
      "Step: 8820000. Mean Reward: 58.62342423619453. Std of Reward: 7.765252327299662.\n",
      "Step: 8850000. Mean Reward: 59.510578209891875. Std of Reward: 8.360688117029966.\n",
      "Step: 8880000. Mean Reward: 57.81276605444669. Std of Reward: 9.476845907294038.\n",
      "Saved Model\n",
      "Step: 8910000. Mean Reward: 57.028417707090696. Std of Reward: 11.464052019775187.\n",
      "Step: 8940000. Mean Reward: 57.195733197037036. Std of Reward: 9.644160724117063.\n",
      "Step: 8970000. Mean Reward: 55.53455503157095. Std of Reward: 8.772346492909403.\n",
      "Step: 9000000. Mean Reward: 57.64663269143928. Std of Reward: 8.976061880739389.\n",
      "Saved Model\n",
      "Step: 9030000. Mean Reward: 57.582287045481266. Std of Reward: 9.237202382058008.\n",
      "Step: 9060000. Mean Reward: 56.70305687969704. Std of Reward: 8.574410245209874.\n",
      "Step: 9090000. Mean Reward: 58.27951197522137. Std of Reward: 8.255471600775927.\n",
      "Step: 9120000. Mean Reward: 56.90145673270152. Std of Reward: 8.893899885959522.\n",
      "Saved Model\n",
      "Step: 9150000. Mean Reward: 56.11475855586687. Std of Reward: 8.141550157871615.\n",
      "Step: 9180000. Mean Reward: 55.41186613805344. Std of Reward: 8.856166014377306.\n",
      "Step: 9210000. Mean Reward: 56.74755800771155. Std of Reward: 8.177676434976451.\n",
      "Step: 9240000. Mean Reward: 56.867095520395516. Std of Reward: 9.079611785571561.\n",
      "Saved Model\n",
      "Step: 9270000. Mean Reward: 57.645593127981556. Std of Reward: 9.155093469378464.\n",
      "Step: 9300000. Mean Reward: 56.982270147574894. Std of Reward: 8.466311785784619.\n",
      "Step: 9330000. Mean Reward: 57.516034975902286. Std of Reward: 8.650333345943105.\n",
      "Step: 9360000. Mean Reward: 58.6656728020007. Std of Reward: 10.077923103687565.\n",
      "Saved Model\n",
      "Step: 9390000. Mean Reward: 58.79200942376712. Std of Reward: 9.58954473888232.\n",
      "Step: 9420000. Mean Reward: 57.85940558459926. Std of Reward: 9.468140904344807.\n",
      "Step: 9450000. Mean Reward: 58.1946993018132. Std of Reward: 8.523792175398789.\n",
      "Step: 9480000. Mean Reward: 59.15820484753499. Std of Reward: 10.427586656987863.\n",
      "Saved Model\n",
      "Step: 9510000. Mean Reward: 57.84064979893469. Std of Reward: 9.05408475724617.\n",
      "Step: 9540000. Mean Reward: 57.23593493506654. Std of Reward: 9.591339597142824.\n",
      "Step: 9570000. Mean Reward: 57.24105883039529. Std of Reward: 8.57474759403126.\n",
      "Step: 9600000. Mean Reward: 57.78286532944995. Std of Reward: 10.42655335744136.\n",
      "Saved Model\n",
      "Step: 9630000. Mean Reward: 56.92092537560594. Std of Reward: 8.038925420647937.\n",
      "Step: 9660000. Mean Reward: 56.316996546799025. Std of Reward: 8.646708969999567.\n",
      "Step: 9690000. Mean Reward: 57.843420961108535. Std of Reward: 8.477797990371572.\n",
      "Step: 9720000. Mean Reward: 57.397596380138474. Std of Reward: 8.997682767393208.\n",
      "Saved Model\n",
      "Step: 9750000. Mean Reward: 56.25397454794067. Std of Reward: 8.6555446900382.\n",
      "Step: 9780000. Mean Reward: 58.566198648127504. Std of Reward: 8.74000202544312.\n",
      "Step: 9810000. Mean Reward: 57.093644183418135. Std of Reward: 8.985575555288717.\n",
      "Step: 9840000. Mean Reward: 57.07667176720719. Std of Reward: 8.934408247681475.\n",
      "Saved Model\n",
      "Step: 9870000. Mean Reward: 56.810258909985194. Std of Reward: 9.07281831615179.\n",
      "Step: 9900000. Mean Reward: 56.93723189350672. Std of Reward: 10.006704632147324.\n",
      "Step: 9930000. Mean Reward: 56.96317105549552. Std of Reward: 9.100262246751889.\n",
      "Step: 9960000. Mean Reward: 57.615768464461475. Std of Reward: 8.818719861983782.\n",
      "Saved Model\n",
      "Step: 9990000. Mean Reward: 56.58540549748961. Std of Reward: 10.242327887686763.\n",
      "Saved Model\n",
      "INFO:tensorflow:Restoring parameters from ./models/deepHinge/model-10000001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/deepHinge/model-10000001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 15 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 15 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/deepHinge/model-10000001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/deepHinge/model-10000001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 15 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 15 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
