{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 1e7 # Set maximum number of steps to run environment.\n",
    "run_path = \"deepHinge4b\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 50000 # Frequency at which to save training statistics.\n",
    "save_freq = 500000 # Frequency at which to save model.\n",
    "env_name = \"build/human\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "# https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-PPO.md\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.995 # Reward discount rate.\n",
    "lambd = 0.900 # Lambda parameter for GAE.\n",
    "time_horizon = 225 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 5e-4 # Strength of entropy regularization. 1e-4 - 1e-2  {1e-2, 1e-4, 2e-4, [5e-4], 1e-3, 5e-3}\n",
    "num_epoch = 6 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 8 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.125 # Acceptable threshold around ratio of old and new policy probabilities. 0.1 - 0.3\n",
    "buffer_size = 327680 # How large the experience buffer should be before gradient descent. 2048 - 409600\n",
    "learning_rate = 1e-4 # Model learning rate. 1e-5 - 1e-3\n",
    "hidden_units = 14 # Number of units in hidden layer.\n",
    "batch_size = 5120 # How many experiences per gradient descent update step. 512 - 5120\n",
    "normalize = False\n",
    "use_recurrent = True\n",
    "sequence_length = 128 # 4 - 128\n",
    "memory_size = 512 # 64 - 512\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size,\n",
    "    'use_recurrent':use_recurrent, 'sequence_length':sequence_length, 'memory_size':memory_size }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 88\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 21\n",
      "        Memory space size (per agent): 128\n",
      "        Action descriptions: hode side, hode rist, hode nikk, rygg bukk, rygg len, l?r v l?p, l?r v strafe, legg v, fot v, l?r h l?p, l?r h strafe, legg h, fot h, skulder v klem, skulder v l?ft, arm v l?ft, arm v vri, skulder h klem, skulder h l?ft, arm h l?ft, arm h vri\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 50000. Mean Reward: 1.3316281287684355. Std of Reward: 34.2053456238516.\n",
      "Step: 100000. Mean Reward: 30.05276301119851. Std of Reward: 18.569473038061062.\n",
      "Step: 150000. Mean Reward: 30.03662543121289. Std of Reward: 14.864338120600877.\n",
      "Step: 200000. Mean Reward: 13.429368507250397. Std of Reward: 9.268826235717533.\n",
      "Step: 250000. Mean Reward: 13.5226435272124. Std of Reward: 9.233137835170742.\n",
      "Step: 300000. Mean Reward: 15.311332268711514. Std of Reward: 10.206235905343059.\n",
      "Step: 350000. Mean Reward: 19.180646855218928. Std of Reward: 11.500471598064598.\n",
      "Step: 400000. Mean Reward: 23.69345073304504. Std of Reward: 16.823381364800614.\n",
      "Step: 450000. Mean Reward: 26.525160765825795. Std of Reward: 22.215497302681516.\n",
      "Step: 500000. Mean Reward: 28.329745287853235. Std of Reward: 23.507738209120692.\n",
      "Saved Model\n",
      "Step: 550000. Mean Reward: 28.700678697427747. Std of Reward: 24.302352200880126.\n",
      "Step: 600000. Mean Reward: 35.67490320828466. Std of Reward: 27.006146256388984.\n",
      "Step: 650000. Mean Reward: 33.587761567709556. Std of Reward: 27.307699839711564.\n",
      "Step: 700000. Mean Reward: 32.87006288464445. Std of Reward: 24.889495387224123.\n",
      "Step: 750000. Mean Reward: 26.041538631317287. Std of Reward: 23.843545618563873.\n",
      "Step: 800000. Mean Reward: 29.846557521334578. Std of Reward: 29.350025113860355.\n",
      "Step: 850000. Mean Reward: 33.92809271416607. Std of Reward: 28.423508203036256.\n",
      "Step: 900000. Mean Reward: 36.241810016920816. Std of Reward: 31.68610917857778.\n",
      "Step: 950000. Mean Reward: 40.71510169413613. Std of Reward: 33.56428315269576.\n",
      "Step: 1000000. Mean Reward: 40.211719571806945. Std of Reward: 32.851603951980344.\n",
      "Saved Model\n",
      "Step: 1050000. Mean Reward: 36.770419868228196. Std of Reward: 31.855162934474983.\n",
      "Step: 1100000. Mean Reward: 45.10431202175597. Std of Reward: 36.480081225441836.\n",
      "Step: 1150000. Mean Reward: 47.66296052746272. Std of Reward: 34.25273669045081.\n",
      "Step: 1200000. Mean Reward: 50.718415833202506. Std of Reward: 33.57351744484906.\n",
      "Step: 1250000. Mean Reward: 49.787751578813676. Std of Reward: 35.067337850121405.\n",
      "Step: 1300000. Mean Reward: 46.479406911454014. Std of Reward: 29.2942438126028.\n",
      "Step: 1350000. Mean Reward: 46.88308335087221. Std of Reward: 32.7471701246454.\n",
      "Step: 1400000. Mean Reward: 47.8108112782781. Std of Reward: 35.439209508444925.\n",
      "Step: 1450000. Mean Reward: 50.07062627765742. Std of Reward: 30.50296556022372.\n",
      "Step: 1500000. Mean Reward: 52.46758875373438. Std of Reward: 33.71334049979891.\n",
      "Saved Model\n",
      "Step: 1550000. Mean Reward: 53.53108092066783. Std of Reward: 32.70836774387983.\n",
      "Step: 1600000. Mean Reward: 51.75030943600176. Std of Reward: 29.580913231174605.\n",
      "Step: 1650000. Mean Reward: 51.871996493919. Std of Reward: 33.38598707373664.\n",
      "Step: 1700000. Mean Reward: 43.719062288552394. Std of Reward: 30.453853029757592.\n",
      "Step: 1750000. Mean Reward: 42.863201625331484. Std of Reward: 38.52209851954293.\n",
      "Step: 1800000. Mean Reward: 48.40862643101437. Std of Reward: 41.20561752288718.\n",
      "Step: 1850000. Mean Reward: 54.038402069803006. Std of Reward: 42.30917079868273.\n",
      "Step: 1900000. Mean Reward: 57.399891712634904. Std of Reward: 41.173245200443645.\n",
      "Step: 1950000. Mean Reward: 62.880926538055554. Std of Reward: 44.03031841300383.\n",
      "Step: 2000000. Mean Reward: 61.66716448563911. Std of Reward: 42.56738762640416.\n",
      "Saved Model\n",
      "Step: 2050000. Mean Reward: 56.71524330421898. Std of Reward: 41.57579585917725.\n",
      "Step: 2100000. Mean Reward: 56.994046280489215. Std of Reward: 41.39795605574344.\n",
      "Step: 2150000. Mean Reward: 55.66935100091212. Std of Reward: 40.19779539675675.\n",
      "Step: 2200000. Mean Reward: 56.55816658659013. Std of Reward: 38.774692885098005.\n",
      "Step: 2250000. Mean Reward: 61.32993291517616. Std of Reward: 43.3550834188504.\n",
      "Step: 2300000. Mean Reward: 53.90473277439964. Std of Reward: 38.63802683864774.\n",
      "Step: 2350000. Mean Reward: 58.788924480387806. Std of Reward: 41.83145226894259.\n",
      "Step: 2400000. Mean Reward: 59.04169434275659. Std of Reward: 42.1165166646055.\n",
      "Step: 2450000. Mean Reward: 57.28696356965042. Std of Reward: 40.996670970027324.\n",
      "Step: 2500000. Mean Reward: 58.12412640878779. Std of Reward: 40.346164916317676.\n",
      "Saved Model\n",
      "Step: 2550000. Mean Reward: 58.79270134388323. Std of Reward: 40.13632685189327.\n",
      "Step: 2600000. Mean Reward: 56.182005063431056. Std of Reward: 37.77961138638456.\n",
      "Step: 2650000. Mean Reward: 62.72656932600159. Std of Reward: 43.86784629721311.\n",
      "Step: 2700000. Mean Reward: 64.00834538096474. Std of Reward: 42.3310679085892.\n",
      "Step: 2750000. Mean Reward: 59.85458706705182. Std of Reward: 40.816384798494305.\n",
      "Step: 2800000. Mean Reward: 60.14892926116963. Std of Reward: 39.378340656156375.\n",
      "Step: 2850000. Mean Reward: 64.08839881640834. Std of Reward: 43.288687230110256.\n",
      "Step: 2900000. Mean Reward: 64.13300978236535. Std of Reward: 40.71456001680723.\n",
      "Step: 2950000. Mean Reward: 62.44735353847152. Std of Reward: 41.14297961856172.\n",
      "Step: 3000000. Mean Reward: 62.46314519217315. Std of Reward: 40.50678744413149.\n",
      "Saved Model\n",
      "Step: 3050000. Mean Reward: 62.76453945637635. Std of Reward: 42.40747540281811.\n",
      "Step: 3100000. Mean Reward: 64.91339286557209. Std of Reward: 41.01882318170985.\n",
      "Step: 3150000. Mean Reward: 63.83101535594429. Std of Reward: 39.76766332709137.\n",
      "Step: 3200000. Mean Reward: 60.47506366577446. Std of Reward: 40.699041236393555.\n",
      "Step: 3250000. Mean Reward: 58.4145345862599. Std of Reward: 39.83048738052268.\n",
      "Step: 3300000. Mean Reward: 68.04499398006608. Std of Reward: 43.14244247333979.\n",
      "Step: 3350000. Mean Reward: 64.98055507733528. Std of Reward: 41.13318350078752.\n",
      "Step: 3400000. Mean Reward: 65.36954482935214. Std of Reward: 42.57589536882304.\n",
      "Step: 3450000. Mean Reward: 65.54061808247202. Std of Reward: 42.101583330390234.\n",
      "Step: 3500000. Mean Reward: 61.24308741221007. Std of Reward: 38.46982386497634.\n",
      "Saved Model\n",
      "Step: 3550000. Mean Reward: 67.37382111456967. Std of Reward: 41.79118442457423.\n",
      "Step: 3600000. Mean Reward: 65.0129049190531. Std of Reward: 37.261165716063786.\n",
      "Step: 3650000. Mean Reward: 67.51864298871065. Std of Reward: 42.237890781047504.\n",
      "Step: 3700000. Mean Reward: 68.68185077400946. Std of Reward: 39.968944131158906.\n",
      "Step: 3750000. Mean Reward: 68.51610624117865. Std of Reward: 42.63815929794002.\n",
      "Step: 3800000. Mean Reward: 66.89273421734089. Std of Reward: 40.53618006887684.\n",
      "Step: 3850000. Mean Reward: 68.03899944310736. Std of Reward: 40.88151290935592.\n",
      "Step: 3900000. Mean Reward: 67.05085730699321. Std of Reward: 40.65577398181704.\n",
      "Step: 3950000. Mean Reward: 67.77427339666501. Std of Reward: 43.350918047538514.\n",
      "Step: 4000000. Mean Reward: 70.41202658206943. Std of Reward: 42.142584638114066.\n",
      "Saved Model\n",
      "Step: 4050000. Mean Reward: 70.95629601186006. Std of Reward: 43.13615162405307.\n",
      "Step: 4100000. Mean Reward: 71.72651227679702. Std of Reward: 41.261504845041635.\n",
      "Step: 4150000. Mean Reward: 71.27604241961393. Std of Reward: 44.83589348460961.\n",
      "Step: 4200000. Mean Reward: 71.68012617540622. Std of Reward: 43.032489885492126.\n",
      "Step: 4250000. Mean Reward: 74.22396487482371. Std of Reward: 43.92576610102654.\n",
      "Step: 4300000. Mean Reward: 72.99163321512702. Std of Reward: 43.845955019540234.\n",
      "Step: 4350000. Mean Reward: 72.63530508819574. Std of Reward: 43.280939927564496.\n",
      "Step: 4400000. Mean Reward: 74.4706105582294. Std of Reward: 41.556148568411416.\n",
      "Step: 4450000. Mean Reward: 73.64045921667719. Std of Reward: 42.1513995019453.\n",
      "Step: 4500000. Mean Reward: 71.87475529231352. Std of Reward: 41.045386236528984.\n",
      "Saved Model\n",
      "Step: 4550000. Mean Reward: 71.60823838350746. Std of Reward: 40.72742316237933.\n",
      "Step: 4600000. Mean Reward: 74.56538702743183. Std of Reward: 44.29477089164038.\n",
      "Step: 4650000. Mean Reward: 73.71428409577608. Std of Reward: 41.76527691317361.\n",
      "Step: 4700000. Mean Reward: 73.10915439435462. Std of Reward: 43.686405481679984.\n",
      "Step: 4750000. Mean Reward: 73.2207368635313. Std of Reward: 42.85361149960789.\n",
      "Step: 4800000. Mean Reward: 71.1620660010592. Std of Reward: 46.13334350497439.\n",
      "Step: 4850000. Mean Reward: 68.64598601549282. Std of Reward: 40.27836331176936.\n",
      "Step: 4900000. Mean Reward: 71.97555674254725. Std of Reward: 42.39778344093567.\n",
      "Step: 4950000. Mean Reward: 72.33267929970204. Std of Reward: 44.662013687273436.\n",
      "Step: 5000000. Mean Reward: 72.1965444003417. Std of Reward: 42.4698270356841.\n",
      "Saved Model\n",
      "Step: 5050000. Mean Reward: 73.18084119609217. Std of Reward: 41.61372170140173.\n",
      "Step: 5100000. Mean Reward: 73.44927444440486. Std of Reward: 42.34768448410735.\n",
      "Step: 5150000. Mean Reward: 75.30142323631252. Std of Reward: 43.90300302192611.\n",
      "Step: 5200000. Mean Reward: 74.51526322335626. Std of Reward: 44.800514215424755.\n",
      "Step: 5250000. Mean Reward: 73.92031028912834. Std of Reward: 45.404068627582056.\n",
      "Step: 5300000. Mean Reward: 75.23928758431508. Std of Reward: 43.78227022941634.\n",
      "Step: 5350000. Mean Reward: 77.49591942113568. Std of Reward: 44.09977933711186.\n",
      "Step: 5400000. Mean Reward: 75.44905708144222. Std of Reward: 41.99081348792205.\n",
      "Step: 5450000. Mean Reward: 74.95602041245688. Std of Reward: 44.67915865909955.\n",
      "Step: 5500000. Mean Reward: 72.71030713111443. Std of Reward: 44.02720430565179.\n",
      "Saved Model\n",
      "Step: 5550000. Mean Reward: 73.7571580292416. Std of Reward: 42.249268800899614.\n",
      "Step: 5600000. Mean Reward: 75.87491940592375. Std of Reward: 43.603845331563996.\n",
      "Step: 5650000. Mean Reward: 77.83280340918779. Std of Reward: 45.14835096532626.\n",
      "Step: 5700000. Mean Reward: 76.81723632963156. Std of Reward: 45.40560646894311.\n",
      "Step: 5750000. Mean Reward: 77.3946443435978. Std of Reward: 45.1908101393031.\n",
      "Step: 5800000. Mean Reward: 77.13178511901573. Std of Reward: 45.62438346185993.\n",
      "Step: 5850000. Mean Reward: 77.44026685674118. Std of Reward: 47.43096185363966.\n",
      "Step: 5900000. Mean Reward: 75.52113692632823. Std of Reward: 43.062464684557696.\n",
      "Step: 5950000. Mean Reward: 77.57039729064165. Std of Reward: 44.197202969632215.\n",
      "Step: 6000000. Mean Reward: 77.2607772053593. Std of Reward: 45.565007443469696.\n",
      "Saved Model\n",
      "Step: 6050000. Mean Reward: 76.22710008414727. Std of Reward: 44.167010114632006.\n",
      "Step: 6100000. Mean Reward: 78.25124267859071. Std of Reward: 46.733184022682586.\n",
      "Step: 6150000. Mean Reward: 79.96042656738756. Std of Reward: 46.64243302874127.\n",
      "Step: 6200000. Mean Reward: 79.21555382707722. Std of Reward: 47.02184437347963.\n",
      "Step: 6250000. Mean Reward: 79.75965375524682. Std of Reward: 45.90799459112559.\n",
      "Step: 6300000. Mean Reward: 80.70699604798851. Std of Reward: 46.15920789989669.\n",
      "Step: 6350000. Mean Reward: 78.42604322619565. Std of Reward: 42.79865553744435.\n",
      "Step: 6400000. Mean Reward: 78.55833519903459. Std of Reward: 43.89186025805101.\n",
      "Step: 6450000. Mean Reward: 78.06162406344683. Std of Reward: 43.3437212398316.\n",
      "Step: 6500000. Mean Reward: 79.92638445242011. Std of Reward: 45.29791784303323.\n",
      "Saved Model\n",
      "Step: 6550000. Mean Reward: 80.77922455552529. Std of Reward: 45.83092039857873.\n",
      "Step: 6600000. Mean Reward: 80.68664472560783. Std of Reward: 45.37110694006177.\n",
      "Step: 6650000. Mean Reward: 81.61858166486611. Std of Reward: 45.72955406831567.\n",
      "Step: 6700000. Mean Reward: 81.59925422501414. Std of Reward: 45.302167970602945.\n",
      "Step: 6750000. Mean Reward: 80.84895968464184. Std of Reward: 45.022095127226684.\n",
      "Step: 6800000. Mean Reward: 81.79243952808295. Std of Reward: 47.266426590270676.\n",
      "Step: 6850000. Mean Reward: 80.94260682527872. Std of Reward: 46.46673269780719.\n",
      "Step: 6900000. Mean Reward: 81.0099853592602. Std of Reward: 46.429905053414025.\n",
      "Step: 6950000. Mean Reward: 82.00730260505486. Std of Reward: 46.33848438894099.\n",
      "Step: 7000000. Mean Reward: 80.52052053949012. Std of Reward: 46.795556069600586.\n",
      "Saved Model\n",
      "Step: 7050000. Mean Reward: 79.78517191543844. Std of Reward: 44.739148900775376.\n",
      "Step: 7100000. Mean Reward: 80.48062145418956. Std of Reward: 44.36997121439401.\n",
      "Step: 7150000. Mean Reward: 80.54102159949097. Std of Reward: 46.61219927087257.\n",
      "Step: 7200000. Mean Reward: 81.1566887386618. Std of Reward: 43.950759527539006.\n",
      "Step: 7250000. Mean Reward: 82.09582216261474. Std of Reward: 45.84733594003285.\n",
      "Step: 7300000. Mean Reward: 82.46859945955987. Std of Reward: 47.675125094615595.\n",
      "Step: 7350000. Mean Reward: 81.12723375989393. Std of Reward: 44.895537051994616.\n",
      "Step: 7400000. Mean Reward: 83.0482980840651. Std of Reward: 46.59205313964105.\n",
      "Step: 7450000. Mean Reward: 83.69379966562921. Std of Reward: 46.86832816944102.\n",
      "Step: 7500000. Mean Reward: 84.01502527455571. Std of Reward: 46.757255246790784.\n",
      "Saved Model\n",
      "Step: 7550000. Mean Reward: 82.76703630861019. Std of Reward: 46.89038804989535.\n",
      "Step: 7600000. Mean Reward: 82.94242624701614. Std of Reward: 46.552264292186045.\n",
      "Step: 7650000. Mean Reward: 83.08383391877723. Std of Reward: 47.8417859909425.\n",
      "Step: 7700000. Mean Reward: 83.60959654143413. Std of Reward: 46.86640446614318.\n",
      "Step: 7750000. Mean Reward: 84.15774645329395. Std of Reward: 46.54209422802144.\n",
      "Step: 7800000. Mean Reward: 84.78172307153426. Std of Reward: 47.798018349402064.\n",
      "Step: 7850000. Mean Reward: 85.20952180639812. Std of Reward: 48.58057680060959.\n",
      "Step: 7900000. Mean Reward: 83.50931723394443. Std of Reward: 48.66515277632943.\n",
      "Step: 7950000. Mean Reward: 82.24914811339387. Std of Reward: 46.80507601883334.\n",
      "Step: 8000000. Mean Reward: 83.06677328082883. Std of Reward: 45.072529052999165.\n",
      "Saved Model\n",
      "Step: 8050000. Mean Reward: 81.70580297907703. Std of Reward: 46.08880282830792.\n",
      "Step: 8100000. Mean Reward: 82.82080622833217. Std of Reward: 48.01594756646877.\n",
      "Step: 8150000. Mean Reward: 81.1574502440534. Std of Reward: 42.96104237944742.\n",
      "Step: 8200000. Mean Reward: 83.59955736288623. Std of Reward: 50.18181912296741.\n",
      "Step: 8250000. Mean Reward: 82.93209284094269. Std of Reward: 45.72156542187963.\n",
      "Step: 8300000. Mean Reward: 85.01317882388976. Std of Reward: 49.435787667055145.\n",
      "Step: 8350000. Mean Reward: 82.19952817832483. Std of Reward: 46.81490922655352.\n",
      "Step: 8400000. Mean Reward: 84.18469968223928. Std of Reward: 48.02544540733585.\n",
      "Step: 8450000. Mean Reward: 83.80419125119342. Std of Reward: 49.28828754432106.\n",
      "Step: 8500000. Mean Reward: 82.89628929842482. Std of Reward: 46.00889887311921.\n",
      "Saved Model\n",
      "Step: 8550000. Mean Reward: 83.81491859288602. Std of Reward: 46.76960847450841.\n",
      "Step: 8600000. Mean Reward: 84.37412915717681. Std of Reward: 46.18128913388013.\n",
      "Step: 8650000. Mean Reward: 82.11251479711038. Std of Reward: 47.56665435864604.\n",
      "Step: 8700000. Mean Reward: 82.83275770457773. Std of Reward: 46.351266985885154.\n",
      "Step: 8750000. Mean Reward: 85.34273137952289. Std of Reward: 49.52009645206858.\n",
      "Step: 8800000. Mean Reward: 84.39636600099595. Std of Reward: 45.981012390320046.\n",
      "Step: 8850000. Mean Reward: 84.44867046027825. Std of Reward: 48.20522625631298.\n",
      "Step: 8900000. Mean Reward: 82.84378316747265. Std of Reward: 47.34551547495662.\n",
      "Step: 8950000. Mean Reward: 82.4452133367429. Std of Reward: 45.407070869719476.\n",
      "Step: 9000000. Mean Reward: 83.31103494527832. Std of Reward: 45.81421827431311.\n",
      "Saved Model\n",
      "Step: 9050000. Mean Reward: 81.7548060456462. Std of Reward: 45.556553678326985.\n",
      "Step: 9100000. Mean Reward: 83.49546142217099. Std of Reward: 49.786166475839046.\n",
      "Step: 9150000. Mean Reward: 83.73946410637342. Std of Reward: 45.31542355833559.\n",
      "Step: 9200000. Mean Reward: 83.30309534519732. Std of Reward: 47.50186698093413.\n",
      "Step: 9250000. Mean Reward: 83.33171768390582. Std of Reward: 45.36585467292073.\n",
      "Step: 9300000. Mean Reward: 83.73256125712922. Std of Reward: 47.72842021335094.\n",
      "Step: 9350000. Mean Reward: 83.14336641166358. Std of Reward: 47.58051991262121.\n",
      "Step: 9400000. Mean Reward: 84.04530140626235. Std of Reward: 48.46948553888784.\n",
      "Step: 9450000. Mean Reward: 83.62811441607089. Std of Reward: 45.986213703725426.\n",
      "Step: 9500000. Mean Reward: 82.47985737383345. Std of Reward: 45.42837239649655.\n",
      "Saved Model\n",
      "Step: 9550000. Mean Reward: 84.35295255376532. Std of Reward: 46.7538624021084.\n",
      "Step: 9600000. Mean Reward: 83.04218939475847. Std of Reward: 46.95346809121288.\n",
      "Step: 9650000. Mean Reward: 84.27832808537937. Std of Reward: 46.446919508306564.\n",
      "Step: 9700000. Mean Reward: 82.78722520092894. Std of Reward: 46.29872331910764.\n",
      "Step: 9750000. Mean Reward: 83.73041177583481. Std of Reward: 45.80015639418916.\n",
      "Step: 9800000. Mean Reward: 83.94350356276522. Std of Reward: 44.8972087151429.\n",
      "Step: 9850000. Mean Reward: 83.60185039341174. Std of Reward: 48.702149922316885.\n",
      "Step: 9900000. Mean Reward: 84.58215001576464. Std of Reward: 47.289437544173474.\n",
      "Step: 9950000. Mean Reward: 84.32373296943945. Std of Reward: 45.84695818739456.\n",
      "Step: 10000000. Mean Reward: 84.14602978385159. Std of Reward: 47.146039999217564.\n",
      "Saved Model\n",
      "Saved Model\n",
      "INFO:tensorflow:Restoring parameters from ./models/deepHinge4b/model-10000001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/deepHinge4b/model-10000001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 19 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 19 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 19 variables to const ops.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "./models/deepHinge4b/build/human.bytes; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fea09393b070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mexport_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/charactercontroller/python/ppo/models.py\u001b[0m in \u001b[0;36mexport_graph\u001b[0;34m(model_path, env_name, target_nodes)\u001b[0m\n\u001b[1;32m     56\u001b[0m                               \u001b[0moutput_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.bytes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                               \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_saver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                               restore_op_name=\"save/restore_all\", filename_tensor_name=\"save/Const:0\")\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py\u001b[0m in \u001b[0;36mfreeze_graph\u001b[0;34m(input_graph, input_saver, input_binary, input_checkpoint, output_node_names, restore_op_name, filename_tensor_name, output_graph, clear_devices, initializer_nodes, variable_names_whitelist, variable_names_blacklist, input_meta_graph, input_saved_model_dir, saved_model_tags, checkpoint_version)\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0minput_saved_model_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m       \u001b[0msaved_model_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m       checkpoint_version=checkpoint_version)\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py\u001b[0m in \u001b[0;36mfreeze_graph_with_def_protos\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moutput_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m       \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0moutput_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, file_content)\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m\"\"\"Writes file_content to the file. Appends to the end of the file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       pywrap_tensorflow.AppendToFile(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_prewrite_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         self._writable_file = pywrap_tensorflow.CreateWritableFile(\n\u001b[0;32m---> 89\u001b[0;31m             compat.as_bytes(self.__name), compat.as_bytes(self.__mode), status)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ./models/deepHinge4b/build/human.bytes; No such file or directory"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/deepHinge4b/model-10000001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/deepHinge4b/model-10000001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 19 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 19 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 19 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
