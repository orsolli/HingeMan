{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 1e7 # Set maximum number of steps to run environment.\n",
    "run_path = \"deepHinge2\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 30000 # Frequency at which to save training statistics.\n",
    "save_freq = 120000 # Frequency at which to save model.\n",
    "env_name = \"build/human\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.995 # Reward discount rate.\n",
    "lambd = 0.900 # Lambda parameter for GAE.\n",
    "time_horizon = 225 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-4 # Strength of entropy regularization\n",
    "num_epoch = 6 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 6 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.125 # Acceptable threshold around ratio of old and new policy probabilities. 0.1 - 0.3\n",
    "buffer_size = 131072 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 5e-4 # Model learning rate. 1e-5 - 1e-3\n",
    "hidden_units = 12 # Number of units in hidden layer.\n",
    "batch_size = 4096 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "use_recurrent = True\n",
    "sequence_length = 128 # 4 - 128\n",
    "memory_size = 512 # 64 - 512\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size,\n",
    "    'use_recurrent':use_recurrent, 'sequence_length':sequence_length, 'memory_size':memory_size }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 88\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 21\n",
      "        Memory space size (per agent): 128\n",
      "        Action descriptions: hode side, hode rist, hode nikk, rygg bukk, rygg len, l?r v l?p, l?r v strafe, legg v, fot v, l?r h l?p, l?r h strafe, legg h, fot h, skulder v klem, skulder v l?ft, arm v l?ft, arm v vri, skulder h klem, skulder h l?ft, arm h l?ft, arm h vri\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 30000. Mean Reward: -15.98843579410669. Std of Reward: 28.910076963788324.\n",
      "Step: 60000. Mean Reward: -18.276564261084587. Std of Reward: 25.536058257182916.\n",
      "Step: 90000. Mean Reward: -20.82045520685747. Std of Reward: 27.69334831236108.\n",
      "Step: 120000. Mean Reward: -17.137286117047864. Std of Reward: 25.57988079176553.\n",
      "Saved Model\n",
      "Step: 150000. Mean Reward: -2.9844845419585972. Std of Reward: 20.580420143417495.\n",
      "Step: 180000. Mean Reward: 3.1037657254905167. Std of Reward: 13.078221101906687.\n",
      "Step: 210000. Mean Reward: 3.184506629887369. Std of Reward: 13.325552552871363.\n",
      "Step: 240000. Mean Reward: 3.314256928390164. Std of Reward: 13.945358419917767.\n",
      "Saved Model\n",
      "Step: 270000. Mean Reward: 6.586217877768073. Std of Reward: 17.17222862327832.\n",
      "Step: 300000. Mean Reward: 27.63780128813141. Std of Reward: 10.534391445041068.\n",
      "Step: 330000. Mean Reward: 26.85874057607911. Std of Reward: 10.467609802508374.\n",
      "Step: 360000. Mean Reward: 27.68143120227237. Std of Reward: 11.383723890429957.\n",
      "Saved Model\n",
      "Step: 390000. Mean Reward: 26.736884597215557. Std of Reward: 9.588324517922253.\n",
      "Step: 420000. Mean Reward: 58.99054144269578. Std of Reward: 28.649232426947908.\n",
      "Step: 450000. Mean Reward: 75.41708535042248. Std of Reward: 24.37735539643738.\n",
      "Step: 480000. Mean Reward: 75.57436065575932. Std of Reward: 22.20633513931273.\n",
      "Saved Model\n",
      "Step: 510000. Mean Reward: 75.7162242288865. Std of Reward: 19.02017539045782.\n",
      "Step: 540000. Mean Reward: 77.46330093464515. Std of Reward: 21.148895160145997.\n",
      "Step: 570000. Mean Reward: 84.99806401685208. Std of Reward: 24.405171712673685.\n",
      "Step: 600000. Mean Reward: 81.9926246653238. Std of Reward: 23.760614833345688.\n",
      "Saved Model\n",
      "Step: 630000. Mean Reward: 81.06159509447109. Std of Reward: 21.39997580693684.\n",
      "Step: 660000. Mean Reward: 84.31453926716235. Std of Reward: 25.18240757435319.\n",
      "Step: 690000. Mean Reward: 100.59242237369251. Std of Reward: 38.219025236585615.\n",
      "Step: 720000. Mean Reward: 100.7342928128648. Std of Reward: 34.66115556037015.\n",
      "Saved Model\n",
      "Step: 750000. Mean Reward: 96.04452994205069. Std of Reward: 32.21515725383611.\n",
      "Step: 780000. Mean Reward: 102.22730029336265. Std of Reward: 37.34501121428754.\n",
      "Step: 810000. Mean Reward: 105.62661370358727. Std of Reward: 41.09887722802214.\n",
      "Step: 840000. Mean Reward: 102.41544947809851. Std of Reward: 37.20909507914503.\n",
      "Saved Model\n",
      "Step: 870000. Mean Reward: 104.46983262934035. Std of Reward: 40.21389435594231.\n",
      "Step: 900000. Mean Reward: 109.44590351739896. Std of Reward: 40.63492417602997.\n",
      "Step: 930000. Mean Reward: 107.92122410156477. Std of Reward: 39.92968947196566.\n",
      "Step: 960000. Mean Reward: 111.13979345608537. Std of Reward: 43.9616150850349.\n",
      "Saved Model\n",
      "Step: 990000. Mean Reward: 108.13774720599594. Std of Reward: 43.76484706625322.\n",
      "Step: 1020000. Mean Reward: 115.62485784320344. Std of Reward: 41.35486644243266.\n",
      "Step: 1050000. Mean Reward: 120.06156731650337. Std of Reward: 47.44053711646703.\n",
      "Step: 1080000. Mean Reward: 104.25871379701186. Std of Reward: 36.83144967083546.\n",
      "Saved Model\n",
      "Step: 1110000. Mean Reward: 103.76172247037226. Std of Reward: 32.78225099472433.\n",
      "Step: 1140000. Mean Reward: 101.25284234734565. Std of Reward: 37.66245534738692.\n",
      "Step: 1170000. Mean Reward: 108.65414425567005. Std of Reward: 39.0966372582349.\n",
      "Step: 1200000. Mean Reward: 97.72237836025343. Std of Reward: 31.10834299330437.\n",
      "Saved Model\n",
      "Step: 1230000. Mean Reward: 102.73838866643662. Std of Reward: 34.78448053003166.\n",
      "Step: 1260000. Mean Reward: 97.4993822948447. Std of Reward: 33.484471230562.\n",
      "Step: 1290000. Mean Reward: 93.99667630529113. Std of Reward: 23.335462106881206.\n",
      "Step: 1320000. Mean Reward: 97.94713413027544. Std of Reward: 31.545236843726588.\n",
      "Saved Model\n",
      "Step: 1350000. Mean Reward: 80.77788167875275. Std of Reward: 12.67756012051202.\n",
      "Step: 1380000. Mean Reward: 80.97508717812917. Std of Reward: 12.421237688687322.\n",
      "Step: 1410000. Mean Reward: 82.88568085970154. Std of Reward: 14.185288085634951.\n",
      "Step: 1440000. Mean Reward: 81.91063618859675. Std of Reward: 11.872605634180433.\n",
      "Saved Model\n",
      "Step: 1470000. Mean Reward: 80.28401201848331. Std of Reward: 11.254515897591833.\n",
      "Step: 1500000. Mean Reward: 79.88793039351182. Std of Reward: 17.188769090006268.\n",
      "Step: 1530000. Mean Reward: 78.32473738985375. Std of Reward: 10.86525050241339.\n",
      "Step: 1560000. Mean Reward: 78.19886744495118. Std of Reward: 10.870678198720839.\n",
      "Saved Model\n",
      "Step: 1590000. Mean Reward: 77.91256386630943. Std of Reward: 10.87669644914691.\n",
      "Step: 1620000. Mean Reward: 72.34969787361345. Std of Reward: 9.208114814869656.\n",
      "Step: 1650000. Mean Reward: 72.36312198755756. Std of Reward: 8.987654039571275.\n",
      "Step: 1680000. Mean Reward: 73.6833767467812. Std of Reward: 12.824708214198326.\n",
      "Saved Model\n",
      "Step: 1710000. Mean Reward: 73.31972325579208. Std of Reward: 12.25365229389717.\n",
      "Step: 1740000. Mean Reward: 71.61382436159278. Std of Reward: 6.7929309333196475.\n",
      "Step: 1770000. Mean Reward: 71.6391651131644. Std of Reward: 7.428170181550242.\n",
      "Step: 1800000. Mean Reward: 71.77486129187693. Std of Reward: 6.25880306332768.\n",
      "Saved Model\n",
      "Step: 1830000. Mean Reward: 71.74419752229471. Std of Reward: 6.607605258993609.\n",
      "Step: 1860000. Mean Reward: 71.618768289298. Std of Reward: 6.446873552236532.\n",
      "Step: 1890000. Mean Reward: 71.16693122291436. Std of Reward: 5.934162364337299.\n",
      "Step: 1920000. Mean Reward: 72.0366198004338. Std of Reward: 6.163384833630916.\n",
      "Saved Model\n",
      "Step: 1950000. Mean Reward: 71.72384711493656. Std of Reward: 6.109384088368797.\n",
      "Step: 1980000. Mean Reward: 72.50233317856156. Std of Reward: 8.992072443060666.\n",
      "Step: 2010000. Mean Reward: 75.66557098333867. Std of Reward: 9.026883303270644.\n",
      "Step: 2040000. Mean Reward: 75.47487878936944. Std of Reward: 8.329143586432533.\n",
      "Saved Model\n",
      "Step: 2070000. Mean Reward: 75.64674302877687. Std of Reward: 5.385798075508103.\n",
      "Step: 2100000. Mean Reward: 75.62661459589657. Std of Reward: 8.8170743180482.\n",
      "Step: 2130000. Mean Reward: 76.17245750444252. Std of Reward: 8.751793139418082.\n",
      "Step: 2160000. Mean Reward: 75.79591796873012. Std of Reward: 6.473605557047247.\n",
      "Saved Model\n",
      "Step: 2190000. Mean Reward: 77.85925037819462. Std of Reward: 13.806901001398227.\n",
      "Step: 2220000. Mean Reward: 76.84264668607346. Std of Reward: 10.010016388396267.\n",
      "Step: 2250000. Mean Reward: 76.58351670638856. Std of Reward: 11.64257379999831.\n",
      "Step: 2280000. Mean Reward: 71.97668271124505. Std of Reward: 5.369816873130622.\n",
      "Saved Model\n",
      "Step: 2310000. Mean Reward: 72.5154369323696. Std of Reward: 7.170800047130126.\n",
      "Step: 2340000. Mean Reward: 73.00737230465542. Std of Reward: 5.932789209704762.\n",
      "Step: 2370000. Mean Reward: 73.2657369655718. Std of Reward: 11.274370563685986.\n",
      "Step: 2400000. Mean Reward: 74.33908266060904. Std of Reward: 7.375072846277619.\n",
      "Saved Model\n",
      "Step: 2430000. Mean Reward: 74.6255696376783. Std of Reward: 5.49310743271686.\n",
      "Step: 2460000. Mean Reward: 74.91415914669128. Std of Reward: 5.806992813662617.\n",
      "Step: 2490000. Mean Reward: 75.21988666173506. Std of Reward: 8.286929915949557.\n",
      "Step: 2520000. Mean Reward: 76.34029549956179. Std of Reward: 6.863414601934336.\n",
      "Saved Model\n",
      "Step: 2550000. Mean Reward: 81.17864174494457. Std of Reward: 12.48910316812552.\n",
      "Step: 2580000. Mean Reward: 79.13014446081172. Std of Reward: 7.294536731341415.\n",
      "Step: 2610000. Mean Reward: 79.93787845243011. Std of Reward: 12.866005320741865.\n",
      "Step: 2640000. Mean Reward: 80.10556575651187. Std of Reward: 12.771579098164018.\n",
      "Saved Model\n",
      "Step: 2670000. Mean Reward: 84.7501463256031. Std of Reward: 19.219598046983702.\n",
      "Step: 2700000. Mean Reward: 89.12406387819168. Std of Reward: 21.92529563083979.\n",
      "Step: 2730000. Mean Reward: 86.68691099962331. Std of Reward: 25.235122210319403.\n",
      "Step: 2760000. Mean Reward: 88.72043815891125. Std of Reward: 24.084278959264886.\n",
      "Saved Model\n",
      "Step: 2790000. Mean Reward: 82.51368439945341. Std of Reward: 15.219419261095078.\n",
      "Step: 2820000. Mean Reward: 79.42381417687989. Std of Reward: 7.512787489035502.\n",
      "Step: 2850000. Mean Reward: 79.13236100270835. Std of Reward: 5.881797435482632.\n",
      "Step: 2880000. Mean Reward: 80.15634494871186. Std of Reward: 10.933831162716176.\n",
      "Saved Model\n",
      "Step: 2910000. Mean Reward: 79.3108462491549. Std of Reward: 7.19627985709329.\n",
      "Step: 2940000. Mean Reward: 78.43578525806228. Std of Reward: 5.010378963359624.\n",
      "Step: 2970000. Mean Reward: 78.39747850984526. Std of Reward: 5.320581692733071.\n",
      "Step: 3000000. Mean Reward: 78.98894160560755. Std of Reward: 5.610826254172997.\n",
      "Saved Model\n",
      "Step: 3030000. Mean Reward: 79.24937357859754. Std of Reward: 6.963622999865886.\n",
      "Step: 3060000. Mean Reward: 81.29696776785575. Std of Reward: 8.7680784060275.\n",
      "Step: 3090000. Mean Reward: 81.25711566124306. Std of Reward: 6.204875053900859.\n",
      "Step: 3120000. Mean Reward: 81.78743060636874. Std of Reward: 8.517723560265907.\n",
      "Saved Model\n",
      "Step: 3150000. Mean Reward: 82.48499398665896. Std of Reward: 8.383114302525794.\n",
      "Step: 3180000. Mean Reward: 84.36192824063336. Std of Reward: 9.56716999507391.\n",
      "Step: 3210000. Mean Reward: 94.34067159424382. Std of Reward: 17.370590584889456.\n",
      "Step: 3240000. Mean Reward: 92.91642239386415. Std of Reward: 10.675895610245384.\n",
      "Saved Model\n",
      "Step: 3270000. Mean Reward: 90.74399726868036. Std of Reward: 11.894058901653091.\n",
      "Step: 3300000. Mean Reward: 91.53644870766205. Std of Reward: 11.741616069384282.\n",
      "Step: 3330000. Mean Reward: 89.44676320858316. Std of Reward: 9.105785195976726.\n",
      "Step: 3360000. Mean Reward: 91.31044801166725. Std of Reward: 14.52917943905889.\n",
      "Saved Model\n",
      "Step: 3390000. Mean Reward: 90.2105903935541. Std of Reward: 11.750406893701237.\n",
      "Step: 3420000. Mean Reward: 90.57685768661449. Std of Reward: 11.729863104810823.\n",
      "Step: 3450000. Mean Reward: 96.96183839032754. Std of Reward: 22.561901435459063.\n",
      "Step: 3480000. Mean Reward: 103.65282601047697. Std of Reward: 26.466025573689397.\n",
      "Saved Model\n",
      "Step: 3510000. Mean Reward: 99.86703793584098. Std of Reward: 23.104726881607668.\n",
      "Step: 3540000. Mean Reward: 103.6382275010793. Std of Reward: 29.687120620773626.\n",
      "Step: 3570000. Mean Reward: 102.04129388708391. Std of Reward: 26.102126316986833.\n",
      "Step: 3600000. Mean Reward: 102.3624933281341. Std of Reward: 30.708444268737427.\n",
      "Saved Model\n",
      "Step: 3630000. Mean Reward: 104.13724055861015. Std of Reward: 32.18229435174186.\n",
      "Step: 3660000. Mean Reward: 104.90032612606726. Std of Reward: 38.18783237916904.\n",
      "Step: 3690000. Mean Reward: 103.53537456442825. Std of Reward: 31.501217987656652.\n",
      "Step: 3720000. Mean Reward: 106.43266626339928. Std of Reward: 28.628151097828948.\n",
      "Saved Model\n",
      "Step: 3750000. Mean Reward: 105.17571659518072. Std of Reward: 24.030200798142086.\n",
      "Step: 3780000. Mean Reward: 107.04223092528986. Std of Reward: 25.608057009334757.\n",
      "Step: 3810000. Mean Reward: 107.75387244926699. Std of Reward: 22.6022435666914.\n",
      "Step: 3840000. Mean Reward: 107.2467518540493. Std of Reward: 25.96781565905659.\n",
      "Saved Model\n",
      "Step: 3870000. Mean Reward: 103.93239076160926. Std of Reward: 19.873014139706505.\n",
      "Step: 3900000. Mean Reward: 103.04618874717623. Std of Reward: 16.78339544773708.\n",
      "Step: 3930000. Mean Reward: 107.24712950254899. Std of Reward: 24.03541136897661.\n",
      "Step: 3960000. Mean Reward: 104.47907346578044. Std of Reward: 25.255997216170478.\n",
      "Saved Model\n",
      "Step: 3990000. Mean Reward: 107.05042651180214. Std of Reward: 39.88186888437585.\n",
      "Step: 4020000. Mean Reward: 115.13034658874071. Std of Reward: 45.07356759731843.\n",
      "Step: 4050000. Mean Reward: 107.23170611615375. Std of Reward: 34.47846163732176.\n",
      "Step: 4080000. Mean Reward: 115.47759442674364. Std of Reward: 43.598869050915155.\n",
      "Saved Model\n",
      "Step: 4110000. Mean Reward: 108.65478796595497. Std of Reward: 40.35740211561495.\n",
      "Step: 4140000. Mean Reward: 109.7085907202383. Std of Reward: 39.318541471005716.\n",
      "Step: 4170000. Mean Reward: 108.83894539834728. Std of Reward: 37.58129075285536.\n",
      "Step: 4200000. Mean Reward: 110.44379642276712. Std of Reward: 36.3243035423145.\n",
      "Saved Model\n",
      "Step: 4230000. Mean Reward: 112.34389348598977. Std of Reward: 40.63987318193422.\n",
      "Step: 4260000. Mean Reward: 115.95378726737968. Std of Reward: 44.179305595009914.\n",
      "Step: 4290000. Mean Reward: 111.71054903973967. Std of Reward: 35.93855800911124.\n",
      "Step: 4320000. Mean Reward: 107.95818994354406. Std of Reward: 34.96055688919047.\n",
      "Saved Model\n",
      "Step: 4350000. Mean Reward: 113.64695100025895. Std of Reward: 43.73806166046433.\n",
      "Step: 4380000. Mean Reward: 106.50738384265652. Std of Reward: 32.14936624865069.\n",
      "Step: 4410000. Mean Reward: 110.01824237116301. Std of Reward: 36.33153750709242.\n",
      "Step: 4440000. Mean Reward: 104.08627365447444. Std of Reward: 35.36286858039019.\n",
      "Saved Model\n",
      "Step: 4470000. Mean Reward: 105.78323941546014. Std of Reward: 35.594409488400366.\n",
      "Step: 4500000. Mean Reward: 107.63068967974942. Std of Reward: 35.69301699862624.\n",
      "Step: 4530000. Mean Reward: 103.78654761880321. Std of Reward: 32.5534122432972.\n",
      "Step: 4560000. Mean Reward: 104.39702335081917. Std of Reward: 36.969451032320265.\n",
      "Saved Model\n",
      "Step: 4590000. Mean Reward: 112.2956045239685. Std of Reward: 41.83435150436715.\n",
      "Step: 4620000. Mean Reward: 109.17708075342597. Std of Reward: 38.4193500803414.\n",
      "Step: 4650000. Mean Reward: 122.85028830155217. Std of Reward: 42.264321302854704.\n",
      "Step: 4680000. Mean Reward: 117.33432308317784. Std of Reward: 42.50666634907616.\n",
      "Saved Model\n",
      "Step: 4710000. Mean Reward: 117.90013522231517. Std of Reward: 41.5621439477731.\n",
      "Step: 4740000. Mean Reward: 121.15331352667538. Std of Reward: 43.911796782643954.\n",
      "Step: 4770000. Mean Reward: 116.06017175181545. Std of Reward: 38.50752914370064.\n",
      "Step: 4800000. Mean Reward: 109.30269901809613. Std of Reward: 37.527942885384505.\n",
      "Saved Model\n",
      "Step: 4830000. Mean Reward: 108.1803195390908. Std of Reward: 38.30094629564032.\n",
      "Step: 4860000. Mean Reward: 115.78598668468814. Std of Reward: 39.336798922727986.\n",
      "Step: 4890000. Mean Reward: 110.36037936718931. Std of Reward: 39.184022201952516.\n",
      "Step: 4920000. Mean Reward: 124.6589680809524. Std of Reward: 41.11973414647554.\n",
      "Saved Model\n",
      "Step: 4950000. Mean Reward: 126.38599992327138. Std of Reward: 46.40898600591236.\n",
      "Step: 4980000. Mean Reward: 126.76266958584935. Std of Reward: 40.524202501442204.\n",
      "Step: 5010000. Mean Reward: 126.59323365211499. Std of Reward: 46.509620018070066.\n",
      "Step: 5040000. Mean Reward: 129.03108133554687. Std of Reward: 42.84725209049036.\n",
      "Saved Model\n",
      "Step: 5070000. Mean Reward: 124.24458767920969. Std of Reward: 37.37863818781671.\n",
      "Step: 5100000. Mean Reward: 131.51561851469233. Std of Reward: 38.700758543031675.\n",
      "Step: 5130000. Mean Reward: 126.47794518676022. Std of Reward: 40.83201766509009.\n",
      "Step: 5160000. Mean Reward: 128.77845141157127. Std of Reward: 40.565262727724786.\n",
      "Saved Model\n",
      "Step: 5190000. Mean Reward: 123.52186469352868. Std of Reward: 42.38107133290701.\n",
      "Step: 5220000. Mean Reward: 118.90325333308685. Std of Reward: 36.62662212640274.\n",
      "Step: 5250000. Mean Reward: 123.73196686609775. Std of Reward: 39.397221573598124.\n",
      "Step: 5280000. Mean Reward: 115.74314466982669. Std of Reward: 37.97857486250176.\n",
      "Saved Model\n",
      "Step: 5310000. Mean Reward: 117.16145265806102. Std of Reward: 41.5389630523305.\n",
      "Step: 5340000. Mean Reward: 116.62630632587255. Std of Reward: 38.45555191395578.\n",
      "Step: 5370000. Mean Reward: 112.34413592276951. Std of Reward: 39.39599372791753.\n",
      "Step: 5400000. Mean Reward: 119.31768887107224. Std of Reward: 50.772114651174604.\n",
      "Saved Model\n",
      "Step: 5430000. Mean Reward: 96.93532608347535. Std of Reward: 28.62629176830457.\n",
      "Step: 5460000. Mean Reward: 89.61324902512541. Std of Reward: 25.082293156283633.\n",
      "Step: 5490000. Mean Reward: 90.42519615762045. Std of Reward: 25.280690204971258.\n",
      "Step: 5520000. Mean Reward: 91.40654175920567. Std of Reward: 25.685118871653955.\n",
      "Saved Model\n",
      "Step: 5550000. Mean Reward: 91.0100149551316. Std of Reward: 25.536968826058025.\n",
      "Step: 5580000. Mean Reward: 92.32804814325897. Std of Reward: 25.40322753621384.\n",
      "Step: 5610000. Mean Reward: 92.69121931321358. Std of Reward: 26.72406764127589.\n",
      "Step: 5640000. Mean Reward: 91.27324687121815. Std of Reward: 25.696362210227164.\n",
      "Saved Model\n",
      "Step: 5670000. Mean Reward: 90.19252337504952. Std of Reward: 24.73198630965253.\n",
      "Step: 5700000. Mean Reward: 98.48687836210966. Std of Reward: 29.5840688726189.\n",
      "Step: 5730000. Mean Reward: 103.81911274205243. Std of Reward: 35.35284539383024.\n",
      "Step: 5760000. Mean Reward: 106.46465249734013. Std of Reward: 34.087425629721416.\n",
      "Saved Model\n",
      "Step: 5790000. Mean Reward: 105.72790891226592. Std of Reward: 36.603675937688685.\n",
      "Step: 5820000. Mean Reward: 105.10625441059948. Std of Reward: 35.27682376676198.\n",
      "Step: 5850000. Mean Reward: 105.35800323362096. Std of Reward: 32.010365673603445.\n",
      "Step: 5880000. Mean Reward: 103.8282758828922. Std of Reward: 35.41718432391339.\n",
      "Saved Model\n",
      "Step: 5910000. Mean Reward: 105.32024325878689. Std of Reward: 30.17946454962982.\n",
      "Step: 5940000. Mean Reward: 107.3418876029285. Std of Reward: 34.82239487308903.\n",
      "Step: 5970000. Mean Reward: 115.15801042379337. Std of Reward: 35.720269283309754.\n",
      "Step: 6000000. Mean Reward: 117.11000655309208. Std of Reward: 35.29641319193597.\n",
      "Saved Model\n",
      "Step: 6030000. Mean Reward: 124.67302172000753. Std of Reward: 43.94984220979528.\n",
      "Step: 6060000. Mean Reward: 112.39349935750047. Std of Reward: 32.69060433091548.\n",
      "Step: 6090000. Mean Reward: 115.30351523330509. Std of Reward: 33.56844167764837.\n",
      "Step: 6120000. Mean Reward: 117.20143200875859. Std of Reward: 35.893671895507964.\n",
      "Saved Model\n",
      "Step: 6150000. Mean Reward: 115.28279694377251. Std of Reward: 33.86523083785236.\n",
      "Step: 6180000. Mean Reward: 118.1801631492774. Std of Reward: 38.88516083577909.\n",
      "Step: 6210000. Mean Reward: 118.89701376531158. Std of Reward: 39.962644150548954.\n",
      "Step: 6240000. Mean Reward: 116.73371528962356. Std of Reward: 35.71931993232033.\n",
      "Saved Model\n",
      "Step: 6270000. Mean Reward: 113.03986461307933. Std of Reward: 36.24366010622355.\n",
      "Step: 6300000. Mean Reward: 108.3106232660004. Std of Reward: 34.31469791950189.\n",
      "Step: 6330000. Mean Reward: 115.11987095386833. Std of Reward: 36.59114993833421.\n",
      "Step: 6360000. Mean Reward: 111.76062731810033. Std of Reward: 35.4686914838422.\n",
      "Saved Model\n",
      "Step: 6390000. Mean Reward: 113.9411502111674. Std of Reward: 38.73104946069725.\n",
      "Step: 6420000. Mean Reward: 109.79559518104215. Std of Reward: 33.33351937813551.\n",
      "Step: 6450000. Mean Reward: 107.49371420507954. Std of Reward: 37.16160785786135.\n",
      "Step: 6480000. Mean Reward: 109.53934113262862. Std of Reward: 35.16248025531448.\n",
      "Saved Model\n",
      "Step: 6510000. Mean Reward: 117.49666764963601. Std of Reward: 40.1573922727856.\n",
      "Step: 6540000. Mean Reward: 113.65272513468244. Std of Reward: 41.89885468479108.\n",
      "Step: 6570000. Mean Reward: 108.50400982038948. Std of Reward: 35.177961220927834.\n",
      "Step: 6600000. Mean Reward: 107.79358046803112. Std of Reward: 34.482879330118024.\n",
      "Saved Model\n",
      "Step: 6630000. Mean Reward: 119.07951126408109. Std of Reward: 34.08675383226827.\n",
      "Step: 6660000. Mean Reward: 117.9446628911337. Std of Reward: 39.24283689392078.\n",
      "Step: 6690000. Mean Reward: 118.65156020745295. Std of Reward: 39.69622029331755.\n",
      "Step: 6720000. Mean Reward: 120.79851273231284. Std of Reward: 40.61625496478945.\n",
      "Saved Model\n",
      "Step: 6750000. Mean Reward: 113.9691485188822. Std of Reward: 40.97862758687846.\n",
      "Step: 6780000. Mean Reward: 117.3829514084637. Std of Reward: 40.58523613162193.\n",
      "Step: 6810000. Mean Reward: 120.61439363710188. Std of Reward: 39.18746081072213.\n",
      "Step: 6840000. Mean Reward: 111.36616640876997. Std of Reward: 34.98201212555629.\n",
      "Saved Model\n",
      "Step: 6870000. Mean Reward: 113.46989187454909. Std of Reward: 38.5558964710035.\n",
      "Step: 6900000. Mean Reward: 102.66700149075028. Std of Reward: 35.33889226395286.\n",
      "Step: 6930000. Mean Reward: 107.96163324414299. Std of Reward: 38.961718762022144.\n",
      "Step: 6960000. Mean Reward: 106.70688171558585. Std of Reward: 37.42635686509382.\n",
      "Saved Model\n",
      "Step: 6990000. Mean Reward: 102.99181477812607. Std of Reward: 36.024641708534254.\n",
      "Step: 7020000. Mean Reward: 106.25720071416565. Std of Reward: 36.777407289671046.\n",
      "Step: 7050000. Mean Reward: 106.8954688567234. Std of Reward: 38.85897054248447.\n",
      "Step: 7080000. Mean Reward: 112.14009423611456. Std of Reward: 42.91579635900024.\n",
      "Saved Model\n",
      "Step: 7110000. Mean Reward: 110.02902544978595. Std of Reward: 38.674761079019255.\n",
      "Step: 7140000. Mean Reward: 106.95469643883986. Std of Reward: 36.910766178351835.\n",
      "Step: 7170000. Mean Reward: 107.33854868226689. Std of Reward: 38.90072130873281.\n",
      "Step: 7200000. Mean Reward: 106.3546892768718. Std of Reward: 37.39420376046612.\n",
      "Saved Model\n",
      "Step: 7230000. Mean Reward: 102.7934740960658. Std of Reward: 36.336590137635234.\n",
      "Step: 7260000. Mean Reward: 104.98991845593123. Std of Reward: 35.18766423806363.\n",
      "Step: 7290000. Mean Reward: 96.22914091851766. Std of Reward: 35.298225320282285.\n",
      "Step: 7320000. Mean Reward: 99.8944182462575. Std of Reward: 34.01284442315048.\n",
      "Saved Model\n",
      "Step: 7350000. Mean Reward: 101.74909944656822. Std of Reward: 37.77029496119478.\n",
      "Step: 7380000. Mean Reward: 99.4073738626873. Std of Reward: 36.36780533130112.\n",
      "Step: 7410000. Mean Reward: 105.17376485110172. Std of Reward: 41.24874397557688.\n",
      "Step: 7440000. Mean Reward: 103.54933420667732. Std of Reward: 36.49642994726062.\n",
      "Saved Model\n",
      "Step: 7470000. Mean Reward: 97.69621958286379. Std of Reward: 31.11324923012036.\n",
      "Step: 7500000. Mean Reward: 104.19993228483871. Std of Reward: 36.81623458433848.\n",
      "Step: 7530000. Mean Reward: 104.56344836361885. Std of Reward: 38.38749687939281.\n",
      "Step: 7560000. Mean Reward: 110.9171939901987. Std of Reward: 39.89832115227461.\n",
      "Saved Model\n",
      "Step: 7590000. Mean Reward: 109.26747204633952. Std of Reward: 41.5339703951614.\n",
      "Step: 7620000. Mean Reward: 107.79120378301505. Std of Reward: 37.2430457563499.\n",
      "Step: 7650000. Mean Reward: 102.29805917657178. Std of Reward: 35.96758775690158.\n",
      "Step: 7680000. Mean Reward: 100.06131258108697. Std of Reward: 34.97689304973073.\n",
      "Saved Model\n",
      "Step: 7710000. Mean Reward: 110.74792564264064. Std of Reward: 46.014334921531734.\n",
      "Step: 7740000. Mean Reward: 107.23994594833957. Std of Reward: 34.70364672942118.\n",
      "Step: 7770000. Mean Reward: 103.4427290434408. Std of Reward: 36.606876300164814.\n",
      "Step: 7800000. Mean Reward: 106.1573004778889. Std of Reward: 38.67950520071869.\n",
      "Saved Model\n",
      "Step: 7830000. Mean Reward: 100.46911095745796. Std of Reward: 39.73833985136569.\n",
      "Step: 7860000. Mean Reward: 110.64123022593606. Std of Reward: 39.06443421772797.\n",
      "Step: 7890000. Mean Reward: 106.60594496950553. Std of Reward: 37.547257877670404.\n",
      "Step: 7920000. Mean Reward: 107.39541259219149. Std of Reward: 48.205956707983205.\n",
      "Saved Model\n",
      "Step: 7950000. Mean Reward: 109.20805977650289. Std of Reward: 42.376884827045295.\n",
      "Step: 7980000. Mean Reward: 104.9421010089957. Std of Reward: 38.15824798280326.\n",
      "Step: 8010000. Mean Reward: 106.6195376614389. Std of Reward: 38.367803927658386.\n",
      "Step: 8040000. Mean Reward: 108.23511313577089. Std of Reward: 41.14107040861948.\n",
      "Saved Model\n",
      "Step: 8070000. Mean Reward: 108.60185581385026. Std of Reward: 37.31456789608056.\n",
      "Step: 8100000. Mean Reward: 112.21786527023991. Std of Reward: 41.00391078230356.\n",
      "Step: 8130000. Mean Reward: 113.76155923841314. Std of Reward: 47.50001222246362.\n",
      "Step: 8160000. Mean Reward: 115.8762768864543. Std of Reward: 45.763342228588854.\n",
      "Saved Model\n",
      "Step: 8190000. Mean Reward: 115.14053632537404. Std of Reward: 46.2647447913252.\n",
      "Step: 8220000. Mean Reward: 110.2323415212723. Std of Reward: 39.9712133634805.\n",
      "Step: 8250000. Mean Reward: 114.09006311330708. Std of Reward: 44.18052804777969.\n",
      "Step: 8280000. Mean Reward: 107.78375557611928. Std of Reward: 37.476511139504396.\n",
      "Saved Model\n",
      "Step: 8310000. Mean Reward: 108.30748245512314. Std of Reward: 35.03528780389846.\n",
      "Step: 8340000. Mean Reward: 111.9458604571427. Std of Reward: 40.46498921838229.\n",
      "Step: 8370000. Mean Reward: 102.34663033755368. Std of Reward: 36.860397943971805.\n",
      "Step: 8400000. Mean Reward: 109.70009058784412. Std of Reward: 40.221996979111964.\n",
      "Saved Model\n",
      "Step: 8430000. Mean Reward: 112.37392693525372. Std of Reward: 41.67288995422138.\n",
      "Step: 8460000. Mean Reward: 111.35096253635415. Std of Reward: 42.10631866298287.\n",
      "Step: 8490000. Mean Reward: 112.40149183939563. Std of Reward: 44.651910624597846.\n",
      "Step: 8520000. Mean Reward: 115.37927261338194. Std of Reward: 41.35036286788906.\n",
      "Saved Model\n",
      "Step: 8550000. Mean Reward: 109.60336612819887. Std of Reward: 40.51284680446234.\n",
      "Step: 8580000. Mean Reward: 112.03311513201588. Std of Reward: 41.95606492599315.\n",
      "Step: 8610000. Mean Reward: 105.18731194371865. Std of Reward: 34.92464846615292.\n",
      "Step: 8640000. Mean Reward: 111.90434107891103. Std of Reward: 42.908197020839616.\n",
      "Saved Model\n",
      "Step: 8670000. Mean Reward: 112.09781448992946. Std of Reward: 41.048922990130464.\n",
      "Step: 8700000. Mean Reward: 104.62968450491154. Std of Reward: 36.30398845370225.\n",
      "Step: 8730000. Mean Reward: 108.6363276851411. Std of Reward: 40.1122365702798.\n",
      "Step: 8760000. Mean Reward: 106.52007116607412. Std of Reward: 38.97433249421934.\n",
      "Saved Model\n",
      "Step: 8790000. Mean Reward: 111.4879243059796. Std of Reward: 41.921148346508936.\n",
      "Step: 8820000. Mean Reward: 106.63856056472456. Std of Reward: 41.270480199467094.\n",
      "Step: 8850000. Mean Reward: 111.06200169982996. Std of Reward: 41.713889972481624.\n",
      "Step: 8880000. Mean Reward: 113.582954810648. Std of Reward: 44.582115888015046.\n",
      "Saved Model\n",
      "Step: 8910000. Mean Reward: 110.11112571583116. Std of Reward: 37.65644113076734.\n",
      "Step: 8940000. Mean Reward: 115.77506643618116. Std of Reward: 46.739798422323084.\n",
      "Step: 8970000. Mean Reward: 112.52819362698241. Std of Reward: 43.35268226143722.\n",
      "Step: 9000000. Mean Reward: 106.83894287177112. Std of Reward: 39.33150767995046.\n",
      "Saved Model\n",
      "Step: 9030000. Mean Reward: 103.30864841139747. Std of Reward: 36.8276331779081.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fea09393b070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Decide and take an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mnew_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/charactercontroller/python/ppo/trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value_estimate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/deepHinge2/model-9000000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/deepHinge2/model-9000000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 15 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 15 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
